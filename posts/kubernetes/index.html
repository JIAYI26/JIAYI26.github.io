<!doctype html><html lang=en dir=auto><head><meta name="referrer" content="no-referrer"><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Kubernetes | JIAYI's Blog</title><meta name=keywords content="Kubernetes"><meta name=description content="Kubernetes是什么？ Kubernetes 是一个生产级别的容器编排平台和集群管理系统。用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。
传统部署时代 早期，各个组织是在物理服务器上运行应用程序。 由于无法限制在物理服务器中运行的应用程序资源使用，因此会导致资源分配问题。 例如，如果在同一台物理服务器上运行多个应用程序， 则可能会出现一个应用程序占用大部分资源的情况，而导致其他应用程序的性能下降。 一种解决方案是将每个应用程序都运行在不同的物理服务器上， 但是当某个应用程式资源利用率不高时，剩余资源无法被分配给其他应用程式， 而且维护许多物理服务器的成本很高。
虚拟化部署时代 因此，虚拟化技术被引入了。虚拟化技术允许你在单个物理服务器的 CPU 上运行多台虚拟机（VM）。 虚拟化能使应用程序在不同 VM 之间被彼此隔离，且能提供一定程度的安全性， 因为一个应用程序的信息不能被另一应用程序随意访问。
虚拟化技术能够更好地利用物理服务器的资源，并且因为可轻松地添加或更新应用程序， 而因此可以具有更高的可扩缩性，以及降低硬件成本等等的好处。 通过虚拟化，你可以将一组物理资源呈现为可丢弃的虚拟机集群。
每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。
容器部署时代 容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。 因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。
容器因具有许多优势而变得流行起来，例如：
 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性）， 提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建、发布时创建应用程序容器镜像，而不是在部署时， 从而将应用程序与基础架构分离。 可观察性：不仅可以显示 OS 级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在笔记本计算机上也可以和在云中运行一样的应用程序。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  Kubernetes的特性 容器是打包和运行应用程序的好方式。在生产环境中， 你需要管理运行着应用程序的容器，并确保服务不会下线。 例如，如果一个容器发生故障，则你需要启动另一个容器。 如果此行为交由给系统处理，是不是会更容易一些？"><meta name=author content="Me"><link rel=canonical href=https://jiayi26.github.io/posts/kubernetes/><link crossorigin=anonymous href=/assets/css/stylesheet.min.b4e19c453811e60acfec1f00c15ac2be1c53f6ab90187e684358ce7faaf48bab.css integrity="sha256-tOGcRTgR5grP7B8AwVrCvhxT9quQGH5oQ1jOf6r0i6s=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.min.2840b7fccd34145847db71a290569594bdbdb00047097f75d6495d162f5d7dff.js integrity="sha256-KEC3/M00FFhH23GikFaVlL29sABHCX911kldFi9dff8=" onload=hljs.initHighlightingOnLoad()></script>
<link rel=icon href=https://xx.mzqcgc.cn/FrbOmxJvq96XDbDPXbiBWM0LU9e3><link rel=icon type=image/png sizes=16x16 href=https://xx.mzqcgc.cn/FrbOmxJvq96XDbDPXbiBWM0LU9e3><link rel=icon type=image/png sizes=32x32 href=https://xx.mzqcgc.cn/FrbOmxJvq96XDbDPXbiBWM0LU9e3><link rel=apple-touch-icon href=https://xx.mzqcgc.cn/FrbOmxJvq96XDbDPXbiBWM0LU9e3><link rel=mask-icon href=https://jiayi26.github.io/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script type=application/javascript>var doNotTrack=!1;doNotTrack||(function(e,o,i,a,t,n,s){e.GoogleAnalyticsObject=t,e[t]=e[t]||function(){(e[t].q=e[t].q||[]).push(arguments)},e[t].l=1*new Date,n=o.createElement(i),s=o.getElementsByTagName(i)[0],n.async=1,n.src=a,s.parentNode.insertBefore(n,s)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="Kubernetes"><meta property="og:description" content="Kubernetes是什么？ Kubernetes 是一个生产级别的容器编排平台和集群管理系统。用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。
传统部署时代 早期，各个组织是在物理服务器上运行应用程序。 由于无法限制在物理服务器中运行的应用程序资源使用，因此会导致资源分配问题。 例如，如果在同一台物理服务器上运行多个应用程序， 则可能会出现一个应用程序占用大部分资源的情况，而导致其他应用程序的性能下降。 一种解决方案是将每个应用程序都运行在不同的物理服务器上， 但是当某个应用程式资源利用率不高时，剩余资源无法被分配给其他应用程式， 而且维护许多物理服务器的成本很高。
虚拟化部署时代 因此，虚拟化技术被引入了。虚拟化技术允许你在单个物理服务器的 CPU 上运行多台虚拟机（VM）。 虚拟化能使应用程序在不同 VM 之间被彼此隔离，且能提供一定程度的安全性， 因为一个应用程序的信息不能被另一应用程序随意访问。
虚拟化技术能够更好地利用物理服务器的资源，并且因为可轻松地添加或更新应用程序， 而因此可以具有更高的可扩缩性，以及降低硬件成本等等的好处。 通过虚拟化，你可以将一组物理资源呈现为可丢弃的虚拟机集群。
每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。
容器部署时代 容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。 因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。
容器因具有许多优势而变得流行起来，例如：
 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性）， 提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建、发布时创建应用程序容器镜像，而不是在部署时， 从而将应用程序与基础架构分离。 可观察性：不仅可以显示 OS 级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在笔记本计算机上也可以和在云中运行一样的应用程序。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  Kubernetes的特性 容器是打包和运行应用程序的好方式。在生产环境中， 你需要管理运行着应用程序的容器，并确保服务不会下线。 例如，如果一个容器发生故障，则你需要启动另一个容器。 如果此行为交由给系统处理，是不是会更容易一些？"><meta property="og:type" content="article"><meta property="og:url" content="https://jiayi26.github.io/posts/kubernetes/"><meta property="article:section" content="posts"><meta property="og:site_name" content="JIAYI's Blog"><meta name=twitter:card content="summary"><meta name=twitter:title content="Kubernetes"><meta name=twitter:description content="Kubernetes是什么？ Kubernetes 是一个生产级别的容器编排平台和集群管理系统。用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。
传统部署时代 早期，各个组织是在物理服务器上运行应用程序。 由于无法限制在物理服务器中运行的应用程序资源使用，因此会导致资源分配问题。 例如，如果在同一台物理服务器上运行多个应用程序， 则可能会出现一个应用程序占用大部分资源的情况，而导致其他应用程序的性能下降。 一种解决方案是将每个应用程序都运行在不同的物理服务器上， 但是当某个应用程式资源利用率不高时，剩余资源无法被分配给其他应用程式， 而且维护许多物理服务器的成本很高。
虚拟化部署时代 因此，虚拟化技术被引入了。虚拟化技术允许你在单个物理服务器的 CPU 上运行多台虚拟机（VM）。 虚拟化能使应用程序在不同 VM 之间被彼此隔离，且能提供一定程度的安全性， 因为一个应用程序的信息不能被另一应用程序随意访问。
虚拟化技术能够更好地利用物理服务器的资源，并且因为可轻松地添加或更新应用程序， 而因此可以具有更高的可扩缩性，以及降低硬件成本等等的好处。 通过虚拟化，你可以将一组物理资源呈现为可丢弃的虚拟机集群。
每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。
容器部署时代 容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。 因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。
容器因具有许多优势而变得流行起来，例如：
 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性）， 提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建、发布时创建应用程序容器镜像，而不是在部署时， 从而将应用程序与基础架构分离。 可观察性：不仅可以显示 OS 级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在笔记本计算机上也可以和在云中运行一样的应用程序。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  Kubernetes的特性 容器是打包和运行应用程序的好方式。在生产环境中， 你需要管理运行着应用程序的容器，并确保服务不会下线。 例如，如果一个容器发生故障，则你需要启动另一个容器。 如果此行为交由给系统处理，是不是会更容易一些？"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"https://jiayi26.github.io/posts/"},{"@type":"ListItem","position":2,"name":"Kubernetes","item":"https://jiayi26.github.io/posts/kubernetes/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Kubernetes","name":"Kubernetes","description":"Kubernetes是什么？ Kubernetes 是一个生产级别的容器编排平台和集群管理系统。用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。\n传统部署时代 早期，各个组织是在物理服务器上运行应用程序。 由于无法限制在物理服务器中运行的应用程序资源使用，因此会导致资源分配问题。 例如，如果在同一台物理服务器上运行多个应用程序， 则可能会出现一个应用程序占用大部分资源的情况，而导致其他应用程序的性能下降。 一种解决方案是将每个应用程序都运行在不同的物理服务器上， 但是当某个应用程式资源利用率不高时，剩余资源无法被分配给其他应用程式， 而且维护许多物理服务器的成本很高。\n虚拟化部署时代 因此，虚拟化技术被引入了。虚拟化技术允许你在单个物理服务器的 CPU 上运行多台虚拟机（VM）。 虚拟化能使应用程序在不同 VM 之间被彼此隔离，且能提供一定程度的安全性， 因为一个应用程序的信息不能被另一应用程序随意访问。\n虚拟化技术能够更好地利用物理服务器的资源，并且因为可轻松地添加或更新应用程序， 而因此可以具有更高的可扩缩性，以及降低硬件成本等等的好处。 通过虚拟化，你可以将一组物理资源呈现为可丢弃的虚拟机集群。\n每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。\n容器部署时代 容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。 因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。\n容器因具有许多优势而变得流行起来，例如：\n 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性）， 提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建、发布时创建应用程序容器镜像，而不是在部署时， 从而将应用程序与基础架构分离。 可观察性：不仅可以显示 OS 级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在笔记本计算机上也可以和在云中运行一样的应用程序。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  Kubernetes的特性 容器是打包和运行应用程序的好方式。在生产环境中， 你需要管理运行着应用程序的容器，并确保服务不会下线。 例如，如果一个容器发生故障，则你需要启动另一个容器。 如果此行为交由给系统处理，是不是会更容易一些？","keywords":["Kubernetes"],"articleBody":"Kubernetes是什么？ Kubernetes 是一个生产级别的容器编排平台和集群管理系统。用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。\n传统部署时代 早期，各个组织是在物理服务器上运行应用程序。 由于无法限制在物理服务器中运行的应用程序资源使用，因此会导致资源分配问题。 例如，如果在同一台物理服务器上运行多个应用程序， 则可能会出现一个应用程序占用大部分资源的情况，而导致其他应用程序的性能下降。 一种解决方案是将每个应用程序都运行在不同的物理服务器上， 但是当某个应用程式资源利用率不高时，剩余资源无法被分配给其他应用程式， 而且维护许多物理服务器的成本很高。\n虚拟化部署时代 因此，虚拟化技术被引入了。虚拟化技术允许你在单个物理服务器的 CPU 上运行多台虚拟机（VM）。 虚拟化能使应用程序在不同 VM 之间被彼此隔离，且能提供一定程度的安全性， 因为一个应用程序的信息不能被另一应用程序随意访问。\n虚拟化技术能够更好地利用物理服务器的资源，并且因为可轻松地添加或更新应用程序， 而因此可以具有更高的可扩缩性，以及降低硬件成本等等的好处。 通过虚拟化，你可以将一组物理资源呈现为可丢弃的虚拟机集群。\n每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。\n容器部署时代 容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。 因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。\n容器因具有许多优势而变得流行起来，例如：\n 敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。 持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性）， 提供可靠且频繁的容器镜像构建和部署。 关注开发与运维的分离：在构建、发布时创建应用程序容器镜像，而不是在部署时， 从而将应用程序与基础架构分离。 可观察性：不仅可以显示 OS 级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。 跨开发、测试和生产的环境一致性：在笔记本计算机上也可以和在云中运行一样的应用程序。 跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。 以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。 松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。 资源隔离：可预测的应用程序性能。 资源利用：高效率和高密度。  Kubernetes的特性 容器是打包和运行应用程序的好方式。在生产环境中， 你需要管理运行着应用程序的容器，并确保服务不会下线。 例如，如果一个容器发生故障，则你需要启动另一个容器。 如果此行为交由给系统处理，是不是会更容易一些？\n这就是 Kubernetes 要来做的事情！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移你的应用、提供部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。\n  服务发现和负载均衡 Kubernetes 可以使用 DNS 名称或自己的 IP 地址来曝露容器。 如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。\n  存储编排 Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。\n  自动部署和回滚 你可以使用 Kubernetes 描述已部署容器的所需状态， 它可以以受控的速率将实际状态更改为期望状态。 例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。\n  自动完成装箱计算 你为 Kubernetes 提供许多节点组成的集群，在这个集群上运行容器化的任务。 你告诉 Kubernetes 每个容器需要多少 CPU 和内存 (RAM)。 Kubernetes 可以将这些容器按实际情况调度到你的节点上，以最佳方式利用你的资源。\n  自我修复 Kubernetes 将重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器， 并且在准备好服务之前不将其通告给客户端。\n  密钥与配置管理 Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。\n  Kubernetes 的基本架构 Kubernetes 采用了现今流行的“控制面 / 数据面”（Control Plane / Data Plane）架构，集群里的计算机被称为“节点”（Node），可以是实机也可以是虚机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用。\n控制面的节点在 Kubernetes 里叫做 Master Node，一般简称为 Master，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。\n数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。\nNode 的数量非常多，构成了一个资源池，Kubernetes 就在这个池里分配资源，调度应用。因为资源被“池化”了，所以管理也就变得比较简单，可以在集群中任意添加或者删除节点。 Master组件 Master组件为集群做出全局决策，比如资源的调度。 以及检测和响应集群事件，例如当不满足部署的 replicas 字段时， 要启动新的 pod）。\n控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件， 并且不会在此计算机上运行用户容器。\napiserver 是整个 Kubernetes 系统的唯一入口，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信，可以说是 Kubernetes 里的联络员。\netcd 是一个高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态，相当于 Kubernetes 里的配置管理员。注意它只与 apiserver 有直接联系，也就是说任何其他组件想要读写 etcd 里的数据都必须经过 apiserver。\nscheduler 负责容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行，相当于部署人员。因为节点状态和 Pod 信息都存储在 etcd 里，所以 scheduler 必须通过 apiserver 才能获得。\ncontroller-manager 负责运行控制器进程。从逻辑上讲， 每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在同一个进程中运行。 负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能，相当于监控运维人员。同样地，它也必须通过 apiserver 获得存储在 etcd 里的信息，才能够实现对资源的各种操作。 这些控制器包括：\n 节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应 任务控制器（Job Controller）：监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成 端点控制器（Endpoints Controller）：填充端点（Endpoints）对象（即加入 Service 与 Pod） 服务帐户和令牌控制器（Service Account \u0026 Token Controllers）：为新的命名空间创建默认帐户和 API 访问令牌  Node 组件 Node组件会在每个节点上运行，负责维护运行的 Pod 并提供 Kubernetes 运行环境。\nkubelet 是 Node 的代理，负责管理 Node 相关的绝大部分操作，Node 上只有它能够与 apiserver 通信，实现状态报告、命令下发、启停容器等功能，相当于是 Node 上的一个“小管家”。\nkube-proxy 的作用有点特别，它是 Node 的网络代理，只负责管理容器的网络通信，简单来说就是为 Pod 转发 TCP/UDP 数据包，相当于是专职的“小邮差”。\ncontainer-runtime 我们就比较熟悉了，它是容器和镜像的实际使用者，在 kubelet 的指挥下创建容器，管理 Pod 的生命周期，是真正干活的“苦力”。\n插件（Addons） 插件使用 Kubernetes 资源（DaemonSet、 Deployment 等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。\nDNS\nDNS在 Kubernetes 集群里实现了域名解析服务，能够让我们以域名而不是 IP 地址的方式来互相通信，是服务发现和负载均衡的基础。由于它对微服务、服务网格等架构至关重要，所以基本上是 Kubernetes 的必备插件。\nDashboard\nDashboard 就是仪表盘。是 Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身， 并进行故障排除。\nKubernetes集群搭建 安装kubeadm  一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令。 每台机器 2 GB 或更多的 RAM（如果少于这个数字将会影响你应用的运行内存）。 CPU 2 核心及以上。 集群中的所有机器的网络彼此均能相互连接（公网和内网都可以）。 节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见这里了解更多详细信息。 开启机器上的某些端口。请参见这里了解更多详细信息。 禁用交换分区。为了保证 kubelet 正常工作，你必须禁用交换分区。  基础环境 这里准备了三台阿里云主机，在三台主机上执行以下操作\n# 安装Docker yum install -y yum-utils  yum-config-manager \\ --add-repo \\ http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6  systemctl enable docker --now  # 配置镜像加速 tee /etc/docker/daemon.json { \"registry-mirrors\": [\"https://5wr347bg.mirror.aliyuncs.com\"], \"exec-opts\": [\"native.cgroupdriver=systemd\"], \"log-driver\": \"json-file\", \"log-opts\": { \"max-size\": \"100m\" }, \"storage-driver\": \"overlay2\" } EOF  systemctl daemon-reload systemctl restart docker  # 设置hostname hostnamectl set-hostname xxx  # 将 SELinux 设置为 permissive 模式（相当于将其禁用） setenforce 0 sed -i 's/^SELINUX=enforcing$/SELINUX=permissive/' /etc/selinux/config  # 关闭swap swapoff -a sed -ri 's/.*swap.*/#\u0026/' /etc/fstab  # 转发 IPv4 并让 iptables 看到桥接流量 cat br_netfilter EOF  # 设置所需的 sysctl 参数，参数在重新启动后保持不变 cat net.bridge.bridge-nf-call-iptables = 1 net.bridge.bridge-nf-call-ip6tables = 1 EOF  # 应用 sysctl 参数而不重新启动 sysctl --system 安装 kubeadm、kubelet 和 kubectl  kubeadm：用来初始化集群的指令。 kubelet：在集群中的每个节点上用来启动 Pod 和容器等。 kubectl：用来与集群通信的命令行工具。  cat [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg exclude=kubelet kubeadm kubectl EOF  yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes=kubernetes  systemctl enable --now kubelet 使用kubeadm引导集群 下载各个机器需要的镜像 tee ./images.sh #!/bin/bash images=( kube-apiserver:v1.20.9 kube-proxy:v1.20.9 kube-controller-manager:v1.20.9 kube-scheduler:v1.20.9 coredns:1.7.0 etcd:3.4.13-0 pause:3.2 ) for imageName in ${images[@]} ; do docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName done EOF  chmod +x ./images.sh \u0026\u0026 ./images.sh 初始化主节点 #所有机器添加master域名映射 echo \"172.21.252.80 cluster-endpoint\"  /etc/hosts  #主节点初始化 kubeadm init \\ --apiserver-advertise-address=172.21.252.80 \\ --control-plane-endpoint=cluster-endpoint \\ --image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images \\ --kubernetes-version v1.20.9 \\ --service-cidr=10.96.0.0/16 \\ --pod-network-cidr=192.168.0.0/16  # 看到这个说明初始化成功了 Your Kubernetes control-plane has initialized successfully!  To start using your cluster, you need to run the following as a regular user:   mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/config  Alternatively, if you are the root user, you can run:   export KUBECONFIG=/etc/kubernetes/admin.conf  You should now deploy a pod network to the cluster. Run \"kubectl apply -f [podnetwork].yaml\" with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/  You can now join any number of control-plane nodes by copying certificate authorities and service account keys on each node and then running the following as root:   kubeadm join cluster-endpoint:6443 --token 6fuiem.x02y9reea0zn32yw \\  --discovery-token-ca-cert-hash sha256:a2dd77dfaf63e2f0554bac26454e5545f89a65a5427d315a2640f3df38a03558 \\  --control-plane  Then you can join any number of worker nodes by running the following on each as root:  kubeadm join cluster-endpoint:6443 --token 6fuiem.x02y9reea0zn32yw \\  --discovery-token-ca-cert-hash sha256:a2dd77dfaf63e2f0554bac26454e5545f89a65a5427d315a2640f3df38a03558  #设置 .kube/config [root@k8s-master ~]# mkdir -p $HOME/.kube [root@k8s-master ~]# sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config [root@k8s-master ~]# sudo chown $(id -u):$(id -g) $HOME/.kube/config 安装网络插件 [root@k8s-master ~]# curl https://docs.projectcalico.org/archive/v3.21/manifests/calico.yaml -O  [root@k8s-master ~]# kubectl apply -f calico.yaml  # 查看集群部署的应用 [root@k8s-master ~]# kubectl get pod -A NAMESPACE NAME READY STATUS RESTARTS AGE kube-system calico-kube-controllers-5bb48c55fd-42txw 1/1 Running 0 3m4s kube-system calico-node-l98kx 1/1 Running 0 3m11s kube-system coredns-5897cd56c4-h8qkb 1/1 Running 0 45m kube-system coredns-5897cd56c4-xm8hv 1/1 Running 0 45m kube-system etcd-k8s-master 1/1 Running 0 46m kube-system kube-apiserver-k8s-master 1/1 Running 0 46m kube-system kube-controller-manager-k8s-master 1/1 Running 0 46m kube-system kube-proxy-df5n2 1/1 Running 0 45m kube-system kube-scheduler-k8s-master 1/1 Running 0 46m  # 查看集群所有节点 [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready control-plane,master 47m v1.20.9  # kubectl命令 [root@k8s-master ~]# kubectl --help kubectl controls the Kubernetes cluster manager.   Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/  Basic Commands (Beginner):  create Create a resource from a file or from stdin.  expose Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service  run Run a particular image on the cluster  set Set specific features on objects  Basic Commands (Intermediate):  explain Documentation of resources  get Display one or many resources  edit Edit a resource on the server  delete Delete resources by filenames, stdin, resources and names, or by resources and label selector  Deploy Commands:  rollout Manage the rollout of a resource  scale Set a new size for a Deployment, ReplicaSet or Replication Controller  autoscale Auto-scale a Deployment, ReplicaSet, or ReplicationController  Cluster Management Commands:  certificate Modify certificate resources.  cluster-info Display cluster info  top Display Resource (CPU/Memory/Storage) usage.  cordon Mark node as unschedulable  uncordon Mark node as schedulable  drain Drain node in preparation for maintenance  taint Update the taints on one or more nodes  Troubleshooting and Debugging Commands:  describe Show details of a specific resource or group of resources  logs Print the logs for a container in a pod  attach Attach to a running container  exec Execute a command in a container  port-forward Forward one or more local ports to a pod  proxy Run a proxy to the Kubernetes API server  cp Copy files and directories to and from containers.  auth Inspect authorization  debug Create debugging sessions for troubleshooting workloads and nodes  Advanced Commands:  diff Diff live version against would-be applied version  apply Apply a configuration to a resource by filename or stdin  patch Update field(s) of a resource  replace Replace a resource by filename or stdin  wait Experimental: Wait for a specific condition on one or many resources.  kustomize Build a kustomization target from a directory or a remote url.  Settings Commands:  label Update the labels on a resource  annotate Update the annotations on a resource  completion Output shell completion code for the specified shell (bash or zsh)  Other Commands:  api-resources Print the supported API resources on the server  api-versions Print the supported API versions on the server, in the form of \"group/version\"  config Modify kubeconfig files  plugin Provides utilities for interacting with plugins.  version Print the client and server version information  Usage:  kubectl [flags] [options]  Use \"kubectl  --help\" for more information about a given command. Use \"kubectl options\" for a list of global command-line options (applies to all commands).  安装网络插件的时候如果报错，说明版本不兼容，可去calico官网查看对应版本。 error: unable to recognize “calico.yaml”: no matches for kind “PodDisruptionBudget” in version “policy/v1”\n 加入node节点 [root@k8s-node1 ~]# kubeadm join cluster-endpoint:6443 --token tevdqf.0dqqjpzye7gtnyj1 \\  --discovery-token-ca-cert-hash sha256:d1f7101180a99cfb7991c1a972e43f8978b3811afc16033907f45cb40c3bdba2  [root@k8s-node2 ~]# kubeadm join cluster-endpoint:6443 --token tevdqf.0dqqjpzye7gtnyj1 \\  --discovery-token-ca-cert-hash sha256:d1f7101180a99cfb7991c1a972e43f8978b3811afc16033907f45cb40c3bdba2  # 再次查看集群所有节点 [root@k8s-master ~]# kubectl get nodes NAME STATUS ROLES AGE VERSION k8s-master Ready control-plane,master 53m v1.20.9 k8s-node1 Ready  94s v1.20.9 k8s-node2 Ready  79s v1.20.9  # 查看名称空间 [root@k8s-master ~]# kubectl get ns NAME STATUS AGE default Active 56m kube-node-lease Active 57m kube-public Active 57m kube-system Active 57m  # 默认情况下，令牌会在 24 小时后过期。如果要在当前令牌过期后将节点加入集群， 则可以通过在控制平面节点上运行以下命令来创建新令牌 kubeadm token create --print-join-command 部署dashboard [root@k8s-master ~]# kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml  # 设置访问端口，把 type: ClusterIP 改为 type: NodePort [root@k8s-master ~]# kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard service/kubernetes-dashboard edited  # 查看暴露的端口 [root@k8s-master ~]# kubectl get svc -A |grep kubernetes-dashboard kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.96.188.220  8000/TCP 85s kubernetes-dashboard kubernetes-dashboard NodePort 10.96.164.146  443:32747/TCP 85s 设置阿里云安全组规则，浏览器随便访问任一节点的https://ip:port\n创建访问账号，准备一个yaml文件\n[root@k8s-master ~]# vi dash.yaml apiVersion: v1 kind: ServiceAccount metadata:  name: admin-user  namespace: kubernetes-dashboard --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:  name: admin-user roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-admin subjects: - kind: ServiceAccount  name: admin-user  namespace: kubernetes-dashboard  [root@k8s-master ~]# kubectl apply -f dash.yaml serviceaccount/admin-user created clusterrolebinding.rbac.authorization.k8s.io/admin-user created 获取访问令牌，登录dashboard\n[root@k8s-master ~]# kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=\"{.secrets[0].name}\") -o go-template=\"{{.data.token | base64decode}}\" eyJhbGciOiJSUzI1NiIsImtpZCI6Ik4yNTdoUzBBWXp4TEFLOVl2R0d3em1udm9tZWR2LTVKLXZLZUpDYXRIU0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW0yZGYyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhMjQ0ODY5Ny00NTA4LTQ0Y2YtYTIyMi0wYzMzYTAwNzk2ZWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.vyC-hgoTrEAu7slM1CtKTZ0j9YZ5_say_ENl1vLTDwyvSMgKcZSbyAzVp7FCb8lSDKcFl6Wy6oVdRKeWwE0i3vJgb0t6LKss-2CqdQXmlZ-zGdK4XFjH4GiIhpSvFOQpYyeqiJQmIB8Hk_Lo0QFZ-xr-y1hVeM0-xxXMnxAznuNrz3nZvn3DaKk0kYwLj8iF9mt0RvNg323mD1J6xo8wtQsj2_oZthcy4cqyv_7bkRzBosN5Xz9GSs9tWqMf8DHOQSrufMh7e64Pf2msP-iwiCAdHcsGtT0FAKp1oGGE14_FAD9K2TCJ4oDOWQlHabckXzrlUJRJshBtjDefYxgvwg 工作负载资源 Pod 什么是Pod Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。\nPod（就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个）容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。\nPod 为其成员容器提供了两种共享资源：网络和存储。\nPod 通常不是直接创建的，而是使用工作负载资源创建。如：Deployments，StatefulSet，DaemonSet，CornJob，Job等。\n使用Pod # 命令行的方式运行 [root@k8s-master ~]# kubectl run nginx --image=nginx pod/nginx created  # -w 可以查看启动过程 [root@k8s-master ~]# kubectl get pod -w NAME READY STATUS RESTARTS AGE nginx 0/1 ContainerCreating 0 16s nginx 1/1 Running 0 25s  # yaml 文件方式运行 [root@k8s-master ~]# vim nginx.yaml apiVersion: v1 kind: Pod metadata:  name: nginx spec:  containers:  - name: nginx  image: nginx:1.14.2  ports:  - containerPort: 80  [root@k8s-master ~]# kubectl apply -f nginx.yaml pod/nginx created  # kubectl get pod 默认查看的是 default 名称空间的 Pod [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 74s  # 查看 Pod 的详细状态，可以看到分配给了node1节点去执行 [root@k8s-master ~]# kubectl describe pod nginx Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal Scheduled 6m16s default-scheduler Successfully assigned default/nginx to k8s-node1  Normal Pulled 6m15s kubelet Container image \"nginx:1.14.2\" already present on machine  Normal Created 6m15s kubelet Created container nginx  Normal Started 6m14s kubelet Started container nginx  # 查看 Pod 的运行日志 [root@k8s-master ~]# kubectl logs nginx  # 查看更详细信息，每个Pod - k8s都会分配一个ip [root@k8s-master ~]# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx 1/1 Running 0 108s 192.168.36.68 k8s-node1    此时的应用还不能外部访问\n Deployment（无状态应用） 专门用来部署应用程序的，能够让应用永不宕机，多用来发布无状态的应用，是 Kubernetes 里最常用也是最有用的一个对象。控制Pod，使Pod拥有多副本，自愈，扩缩容等能力。\n# 用run启一个pod  [root@k8s-master ~]# kubectl run nginx --image=nginx pod/nginx created  # 用 create deployment 启一个pod [root@k8s-master ~]# kubectl create deployment tomcat --image=tomcat deployment.apps/tomcat created  [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx 1/1 Running 0 2m24s tomcat-7d987c7694-gwmjr 1/1 Running 0 102s  # nginx 删了就没了，tomcat 删了后会重新启动一个tomcat [root@k8s-master ~]# kubectl delete pod nginx pod \"nginx\" deleted [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE tomcat-7d987c7694-gwmjr 1/1 Running 0 2m24s  # deployment 的自愈能力 [root@k8s-master ~]# kubectl delete pod tomcat-7d987c7694-gwmjr pod \"tomcat-7d987c7694-gwmjr\" deleted [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE tomcat-7d987c7694-78b2d 1/1 Running 0 42s  # 删除 [root@k8s-master ~]# kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE tomcat 1/1 1 1 7m38s [root@k8s-master ~]# kubectl delete deploy tomcat deployment.apps \"tomcat\" deleted [root@k8s-master ~]# kubectl get deploy No resources found in default namespace.  kubectl api-resources 可以显示 k8s 的资源信息\n 多副本 # 命令行的方式 [root@k8s-master ~]# kubectl create deployment nginx --image=nginx --replicas=3 deployment.apps/nginx created  [root@k8s-master ~]# kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx 3/3 3 3 60s  [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-bxptc 1/1 Running 0 62s nginx-6799fc88d8-fq296 1/1 Running 0 62s nginx-6799fc88d8-gjd8c 1/1 Running 0 62s  # yaml文件 [root@k8s-master ~]# vim nginx-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: nginx-deployment  labels:  app: nginx spec:  replicas: 3  selector:  matchLabels:  app: nginx  template:  metadata:  labels:  app: nginx  spec:  containers:  - name: nginx  image: nginx:1.14.2  ports:  - containerPort: 80 到dashboard上看到刚刚创建的pod，也可以直接在dashboard上创建。\n扩缩容 # 扩容 [root@k8s-master ~]# kubectl scale deployment/nginx --replicas=5 deployment.apps/nginx scaled  [root@k8s-master ~]# kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx 5/5 5 5 21m  [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-7r6tz 1/1 Running 0 63s nginx-6799fc88d8-bxptc 1/1 Running 0 21m nginx-6799fc88d8-fq296 1/1 Running 0 21m nginx-6799fc88d8-gjd8c 1/1 Running 0 21m nginx-6799fc88d8-vsfzs 1/1 Running 0 63s  # 缩容 [root@k8s-master ~]# kubectl scale deployment/nginx --replicas=2 deployment.apps/nginx scaled  [root@k8s-master ~]# kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx 2/2 2 2 23m  [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-7r6tz 1/1 Running 0 2m23s nginx-6799fc88d8-vsfzs 1/1 Running 0 2m23s  # 也可以直接用 edit 的方式，修改 replicas的副本数 [root@k8s-master ~]# kubectl edit deployment nginx dashboard上操作\n自愈\u0026故障转移 [root@k8s-master ~]# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-6799fc88d8-7r6tz 1/1 Running 0 13m 192.168.36.71 k8s-node1   nginx-6799fc88d8-p9fw5 1/1 Running 0 7m1s 192.168.169.142 k8s-node2   nginx-6799fc88d8-vsfzs 1/1 Running 0 13m 192.168.36.70 k8s-node1    # 把node1上的容器停掉 [root@k8s-node1 ~]# docker stop 11498d366d4d  # 观察 pod 的变化 [root@k8s-master ~]# kubectl get pod -w NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-7r6tz 1/1 Running 0 14m nginx-6799fc88d8-p9fw5 1/1 Running 0 7m59s nginx-6799fc88d8-vsfzs 1/1 Running 0 14m nginx-6799fc88d8-7r6tz 0/1 Completed 0 14m nginx-6799fc88d8-7r6tz 1/1 Running 1 14m  # RESTARTS 变成1 [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-7r6tz 1/1 Running 1 19m nginx-6799fc88d8-p9fw5 1/1 Running 0 12m nginx-6799fc88d8-vsfzs 1/1 Running 0 19m  # 假设node2节点宕机了 [root@k8s-master ~]# kubectl get pod -w NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-7r6tz 1/1 Running 1 18m nginx-6799fc88d8-p9fw5 1/1 Running 0 12m nginx-6799fc88d8-vsfzs 1/1 Running 0 18m nginx-6799fc88d8-p9fw5 1/1 Running 0 13m nginx-6799fc88d8-p9fw5 1/1 Terminating 0 18m nginx-6799fc88d8-ghdsl 0/1 Pending 0 0s nginx-6799fc88d8-ghdsl 0/1 Pending 0 0s nginx-6799fc88d8-ghdsl 0/1 ContainerCreating 0 0s nginx-6799fc88d8-ghdsl 0/1 ContainerCreating 0 1s nginx-6799fc88d8-ghdsl 1/1 Running 0 17s  # 转移到了node1节点 [root@k8s-master ~]# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-6799fc88d8-7r6tz 1/1 Running 1 25m 192.168.36.71 k8s-node1   nginx-6799fc88d8-ghdsl 1/1 Running 0 65s 192.168.36.72 k8s-node1   nginx-6799fc88d8-p9fw5 1/1 Terminating 0 19m 192.168.169.142 k8s-node2   nginx-6799fc88d8-vsfzs 1/1 Running 0 25m 192.168.36.70 k8s-node1   滚动更新 [root@k8s-master ~]# kubectl set image deployment/nginx nginx=nginx:1.16.1 --record deployment.apps/nginx image updated  # 滚动更新的过程 [root@k8s-master ~]# kubectl get pod -w NAME READY STATUS RESTARTS AGE nginx-6799fc88d8-2s6k7 1/1 Running 0 42s nginx-6799fc88d8-4tx9q 1/1 Running 0 42s nginx-6799fc88d8-g2jsk 1/1 Running 0 42s nginx-6889dfccd5-76299 0/1 ContainerCreating 0 3s nginx-6889dfccd5-76299 1/1 Running 0 17s nginx-6799fc88d8-4tx9q 1/1 Terminating 0 56s nginx-6889dfccd5-p4d54 0/1 Pending 0 0s nginx-6889dfccd5-p4d54 0/1 Pending 0 0s nginx-6889dfccd5-p4d54 0/1 ContainerCreating 0 0s nginx-6799fc88d8-4tx9q 1/1 Terminating 0 56s nginx-6799fc88d8-4tx9q 0/1 Terminating 0 57s nginx-6889dfccd5-p4d54 0/1 ContainerCreating 0 1s nginx-6799fc88d8-4tx9q 0/1 Terminating 0 58s nginx-6799fc88d8-4tx9q 0/1 Terminating 0 67s nginx-6799fc88d8-4tx9q 0/1 Terminating 0 67s nginx-6889dfccd5-p4d54 1/1 Running 0 17s nginx-6799fc88d8-g2jsk 1/1 Terminating 0 73s nginx-6889dfccd5-55w2h 0/1 Pending 0 0s nginx-6889dfccd5-55w2h 0/1 Pending 0 0s nginx-6889dfccd5-55w2h 0/1 ContainerCreating 0 0s nginx-6799fc88d8-g2jsk 1/1 Terminating 0 74s nginx-6799fc88d8-g2jsk 0/1 Terminating 0 74s nginx-6889dfccd5-55w2h 0/1 ContainerCreating 0 1s nginx-6799fc88d8-g2jsk 0/1 Terminating 0 77s nginx-6799fc88d8-g2jsk 0/1 Terminating 0 77s nginx-6889dfccd5-55w2h 1/1 Running 0 17s nginx-6799fc88d8-2s6k7 1/1 Terminating 0 90s nginx-6799fc88d8-2s6k7 1/1 Terminating 0 91s nginx-6799fc88d8-2s6k7 0/1 Terminating 0 91s nginx-6799fc88d8-2s6k7 0/1 Terminating 0 92s nginx-6799fc88d8-2s6k7 0/1 Terminating 0 92s  [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE nginx-6889dfccd5-55w2h 1/1 Running 0 21s nginx-6889dfccd5-76299 1/1 Running 0 55s nginx-6889dfccd5-p4d54 1/1 Running 0 38s 版本回退 # 查看历史记录 [root@k8s-master ~]# kubectl rollout history deployment/nginx deployment.apps/nginx REVISION CHANGE-CAUSE 1  2 kubectl set image deployment/nginx nginx=nginx:1.16.1 --record=true  # 查看历史详情 [root@k8s-master ~]# kubectl rollout history deployment/nginx --revision=2 deployment.apps/nginx with revision #2 Pod Template:  Labels:\tapp=nginx \tpod-template-hash=6889dfccd5  Annotations:\tkubernetes.io/change-cause: kubectl set image deployment/nginx nginx=nginx:1.16.1 --record=true  Containers:  nginx:  Image:\tnginx:1.16.1  Port:\t  Host Port:\t  Environment:\t  Mounts:\t  Volumes:\t  # 回退到上个版本，也可以指定 --reversion=2 [root@k8s-master ~]# kubectl rollout undo deployment/nginx deployment.apps/nginx rolled back  # 查看已经回退到了上个版本 [root@k8s-master ~]# kubectl get deploy/nginx -oyaml |grep image  f:imagePullPolicy: {}  f:image: {}  - image: nginx  imagePullPolicy: Always DaemonSet DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。在形式上和 Deployment 类似，都是管理控制 Pod，但管理调度策略却不同。\nDaemonSet 的一些典型用法：\n 网络应用（如 kube-proxy），必须每个节点都运行一个 Pod，否则节点就无法加入 Kubernetes 网络。 监控应用（如 Prometheus），必须每个节点都有一个 Pod，用来监控节点的状态，实时上报信息。 日志应用（如 Fluentd），必须在每个节点上运行一个 Pod，才能够搜集容器运行时产生的日志数据。  kubectl 不提供自动创建 DaemonSet YAML 样板的功能，也就是说，我们不能用命令 kubectl create 直接创建出一个 DaemonSet 对象。\n使用DaemonSet [root@k8s-master ~]# kubectl create ds Error: must specify one of -f and -k  error: unknown command \"ds\" See 'kubectl create -h' for help and examples DaemonSet 的YAML文件和 Deployment 的YAML文件是几乎一模一样的，差别在于DaemonSet 的YAML文件中没有 replicas 字段，所以我们可以先用 kubectl create deploy 创建一个 Deployment 对象，再把 kind 改成 DaemonSet，删除 spec.replicas 就行了。\n[root@k8s-master ~]# kubectl create deploy redis-ds --image=redis:5-alpine --dry-run=client -o yaml apiVersion: apps/v1 kind: Deployment metadata:  creationTimestamp: null  labels:  app: redis-ds  name: redis-ds spec:  replicas: 1  selector:  matchLabels:  app: redis-ds  strategy: {}  template:  metadata:  creationTimestamp: null  labels:  app: redis-ds  spec:  containers:  - image: redis:5-alpine  name: redis  resources: {} status: {}  # 修改以后变成以下 ds.yaml [root@k8s-master ~]# cat ds.yaml  apiVersion: apps/v1 kind: DaemonSet metadata:  labels:  app: redis-ds  name: redis-ds spec:  selector:  matchLabels:  app: redis-ds  template:  metadata:  labels:  app: redis-ds  spec:  containers:  - image: redis:5-alpine  name: redis  ports:  - containerPort: 6379  [root@k8s-master ~]# kubectl apply -f ds.yaml  daemonset.apps/redis-ds created  # 看到生成了两个Pod，运行在node节点上，Master默认是不跑应用的 [root@k8s-master ~]# kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE redis-ds 2 2 1 2 1  5s  [root@k8s-master ~]# kubectl get pod -owide redis-ds-9vwjg 1/1 Running 0 2m23s 192.168.169.132 k8s-node2   redis-ds-z47hg 1/1 Running 0 2m23s 192.168.36.68 k8s-node1   DaemonSet 应该在每个节点上都运行一个 Pod 实例才对，但 Master 节点却被排除在外了，这就不符合我们当初的设想了。为了应对 Pod 在某些节点的“调度”和“驱逐”问题，它定义了两个新的概念：污点（taint）和容忍度（toleration）。\n污点（taint）和容忍度（toleration） “污点”是 Kubernetes 节点的一个属性，它的作用也是给节点“贴标签”，但为了不和已有的 labels 字段混淆，就改成了 taint。\n和“污点”相对的，就是 Pod 的“容忍度”，顾名思义，就是 Pod 能否“容忍”污点。\nKubernetes 在创建集群的时候会自动给节点 Node 加上一些“污点”，方便 Pod 的调度和部署。你可以用 kubectl describe node 来查看 Master 和 Node 的状态：\n[root@k8s-master ~]# kubectl describe node Name: k8s-master Roles: control-plane,master ... Taints: node-role.kubernetes.io/master:NoSchedule ...  Name: k8s-node1 Roles:  ... Taints:  ...  Name: k8s-node2 Roles:  ... Taints:  ... 可以看到，Master 节点默认有一个 taint，名字是 node-role.kubernetes.io/master，它的效果是 NoSchedule，也就是说这个污点会拒绝 Pod 调度到本节点上运行，而 Node 节点的 taint 字段则是空的。\n这正是 Master 和 Worker 在 Pod 调度策略上的区别所在，通常来说 Pod 都不能容忍任何“污点”，所以加上了 taint 属性的 Master 节点也就会无缘 Pod 了。\n让 DaemonSet 在 Master 节点（或者任意其他节点）上运行了，方法有两种：\n  去掉 Master 节点上的 taint\n  为 Pod 添加字段 tolerations，让它能够“容忍”某些“污点”。\n  # 第一种方法 [root@k8s-master ~]# kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule- node/k8s-master untainted  [root@k8s-master ~]# kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE redis-ds 3 3 3 3 3  21m [root@k8s-master ~]# kubectl get pod -owide redis-ds-672rq 1/1 Running 0 30s 192.168.235.196 k8s-master   redis-ds-9vwjg 1/1 Running 0 21m 192.168.169.132 k8s-node2   redis-ds-z47hg 1/1 Running 0 21m 192.168.36.68 k8s-node1    # 第二种方法 # 先把Master的“污点”加上 [root@k8s-master ~]# kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule  # ds.yaml 里的 ds.spec.template.spec.tolerations 加入以下字段 tolerations: - key: node-role.kubernetes.io/master  effect: NoSchedule  operator: Exists  # 重新部署加上了“容忍度”的 DaemonSet [root@k8s-master ~]# kubectl apply -f ds.yaml daemonset.apps/redis-ds created  # 看到仍然还是有3个Pod分别运行在每个节点上 [root@k8s-master ~]# kubectl get ds NAME DESIRED CURRENT READY UP-TO-DATE AVAILABLE NODE SELECTOR AGE redis-ds 3 3 3 3 3  13s  [root@k8s-master ~]# kubectl get pod -owide redis-ds-7l5j5 1/1 Running 0 20s 192.168.235.197 k8s-master   redis-ds-b2h4b 1/1 Running 0 20s 192.168.36.69 k8s-node1   redis-ds-gljb4 1/1 Running 0 20s 192.168.169.133 k8s-node2   StatefulSet 服务发现与负载均衡 Service Kubernetes 中 Pod 的生命周期是比较“短暂”的，虽然 Deployment 和 DaemonSet 可以维持 Pod 总体数量的稳定，但在运行过程中，难免会有 Pod 销毁又重建，这就会导致 Pod 集合处于动态的变化之中。\n这导致了一个问题： 如果一组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用提供工作负载的后端部分？\nKubernetes 中 Service 是将运行在一个或一组 Pod 上的网络应用程序公开为网络服务的方法。\nService 本身是没有服务能力的，它只是一些 iptables 规则，真正配置、应用这些规则的实际上是节点里的 kube-proxy 组件。\nService 的工作原理示意图：\n# 修改每个nginx的/usr/share/nginx/html/index.html文件 [root@k8s-master ~]# kubectl get pod -owide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES nginx-6799fc88d8-5sdkc 1/1 Running 0 19m 192.168.169.130 k8s-node2   nginx-6799fc88d8-s9wgx 1/1 Running 0 19m 192.168.36.65 k8s-node1   nginx-6799fc88d8-w6q4p 1/1 Running 0 19m 192.168.169.129 k8s-node2   [root@k8s-master ~]# curl 192.168.169.130 1111 [root@k8s-master ~]# curl 192.168.36.65 2222 [root@k8s-master ~]# curl 192.168.169.129 3333  # --dry-run=client -o yaml 以yaml的形式输出 [root@k8s-master ~]# kubectl expose deploy nginx --port=8000 --target-port=80 --dry-run=client -o yaml apiVersion: v1 kind: Service metadata:  creationTimestamp: null  labels:  app: nginx  name: nginx spec:  ports:  - port: 8000  protocol: TCP  targetPort: 80  selector:  app: nginx status:  loadBalancer: {}  # 暴露 deploy [root@k8s-master ~]# kubectl expose deploy nginx --port=8000 --target-port=80 service/nginx exposed  [root@k8s-master ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1  443/TCP 34m nginx ClusterIP 10.96.103.120  8000/TCP 12s  # 集群内使用 CLUSTER-IP 访问 [root@k8s-master ~]# curl 10.96.103.120:8000 3333 [root@k8s-master ~]# curl 10.96.103.120:8000 3333 [root@k8s-master ~]# curl 10.96.103.120:8000 3333 [root@k8s-master ~]# curl 10.96.103.120:8000 2222 [root@k8s-master ~]# curl 10.96.103.120:8000 1111 [root@k8s-master ~]# curl 10.96.103.120:8000 1111 ClusterIP –type=ClusterIP 集群内部访问\n# 默认是 ClusterIP ，也可以加上 --type=ClusterIP  [root@k8s-master ~]# kubectl expose deploy nginx --port=8000 --target-port=80 --type=ClusterIP --dry-run=client -o yaml apiVersion: v1 kind: Service metadata:  creationTimestamp: null  labels:  app: nginx  name: nginx spec:  ports:  - port: 8000  protocol: TCP  targetPort: 80  selector:  app: nginx  type: ClusterIP status:  loadBalancer: {} NodePort –type=NodePort 集群外部也可以访问\n[root@k8s-master ~]# kubectl expose deploy nginx --port=8000 --target-port=80 --type=NodePort --target-port=80 --dry-run=client -o yaml apiVersion: v1 kind: Service metadata:  creationTimestamp: null  labels:  app: nginx  name: nginx spec:  ports:  - port: 8000  protocol: TCP  targetPort: 80  selector:  app: nginx  type: NodePort status:  loadBalancer: {}  # PORT 那里多了个32018的端口 [root@k8s-master ~]# kubectl expose deploy nginx --port=8000 --target-port=80 --type=NodePort --target-port=80 service/nginx exposed [root@k8s-master ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1  443/TCP 58m nginx NodePort 10.96.64.157  8000:32018/TCP 8s  # 浏览器访问任一节点的公网IP:32018  NodePort 范围在 30000-32767 之间\n Ingress Ingress 公开从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。\nService 本质上就是一个由 kube-proxy 控制的四层负载均衡，在 TCP/IP 协议栈上转发流量。但在四层上的负载均衡功能还是太有限了，只能够依据 IP 地址和端口号做一些简单的判断和组合，而我们现在的绝大多数应用都是跑在七层的 HTTP/HTTPS 协议上的，有更多的高级路由条件，比如主机名、URI、请求头、证书等等，而这些在 TCP/IP 网络栈里是根本看不见的。\n于是就引入了 Ingress，它是在七层上做负载均衡，除了七层负载均衡，也作为流量的总入口，统管集群的进出口数据。\nIngress 也只是一些 HTTP 路由规则的集合，相当于一份静态的描述文件，真正要把这些规则在集群里实施运行，还需要有另外一个东西，这就是 Ingress Controller，它的作用就相当于 Service 的 kube-proxy，能够读取、应用 Ingress 规则，处理、调度流量。\n安装 [root@k8s-master ~]# wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml  [root@k8s-master ~]# kubectl apply -f deploy.yaml  [root@k8s-master ~]# kubectl get svc -A NAMESPACE NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE default kubernetes ClusterIP 10.96.0.1  443/TCP 18h default nginx NodePort 10.96.64.157  8000:32018/TCP 18h ingress-nginx ingress-nginx-controller NodePort 10.96.177.2  80:31043/TCP,443:31379/TCP 13m ingress-nginx ingress-nginx-controller-admission ClusterIP 10.96.194.45  443/TCP 13m kube-system kube-dns ClusterIP 10.96.0.10  53/UDP,53/TCP,9153/TCP 18h kubernetes-dashboard dashboard-metrics-scraper ClusterIP 10.96.188.220  8000/TCP 18h kubernetes-dashboard kubernetes-dashboard NodePort 10.96.164.146  443:32747/TCP 18h 浏览器访问任一节点的 http://公网IP:31043，https://公网IP:31379\n测试域名访问 tomcat.aliyun.com:31043请求转给tomcat-demo处理\nnginx.aliyun.com:31043请求转给nginx-demo处理\n# 准备一个test.yaml的文件 [root@k8s-master ~]# cat test.yaml  apiVersion: apps/v1 kind: Deployment metadata:  labels:  app: tomcat-demo  name: tomcat-demo spec:  replicas: 2  selector:  matchLabels:  app: tomcat-demo  template:  metadata:  labels:  app: tomcat-demo  spec:  containers:  - name: tomcat  image: tomcat --- apiVersion: apps/v1 kind: Deployment metadata:  labels:  app: nginx-demo  name: nginx-demo spec:  replicas: 2  selector:  matchLabels:  app: nginx-demo  template:  metadata:  labels:  app: nginx-demo  spec:  containers:  - image: nginx  name: nginx --- apiVersion: v1 kind: Service metadata:  labels:  app: nginx-demo  name: nginx-demo spec:  selector:  app: nginx-demo  ports:  - port: 8000  protocol: TCP  targetPort: 80 --- apiVersion: v1 kind: Service metadata:  labels:  app: tomcat-demo  name: tomcat-demo spec:  selector:  app: tomcat-demo  ports:  - port: 8000  protocol: TCP  targetPort: 9000  # ingress.yaml文件 [root@k8s-master ~]# cat ingress.yaml  apiVersion: networking.k8s.io/v1 kind: Ingress  metadata:  name: ingress-host-bar spec:  ingressClassName: nginx  rules:  - host: \"tomcat.aliyun.com\"  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: tomcat-demo  port:  number: 8000  - host: \"nginx.aliyun.com\"  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: nginx-demo  port:  number: 8000  # 本地hosts文件新增 公网IP\ttomcat.aliyun.com 公网IP\tnginx.aliyun.com  [root@k8s-master ~]# kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1  443/TCP 20h nginx NodePort 10.96.64.157  8000:32018/TCP 19h nginx-demo ClusterIP 10.96.161.217  8000/TCP 12s tomcat-demo ClusterIP 10.96.149.244  8000/TCP 12s  [root@k8s-master ~]# kubectl get deploy NAME READY UP-TO-DATE AVAILABLE AGE nginx-demo 2/2 2 2 20s tomcat-demo 1/2 2 1 20s  [root@k8s-master ~]# kubectl get ing NAME CLASS HOSTS ADDRESS PORTS AGE ingress-host-bar nginx tomcat.aliyun.com,nginx.aliyun.com 80 18s 浏览器访问域名:31043\n路径重写 官网地址有详细介绍\n[root@k8s-master ~]# cat ingress.yaml apiVersion: networking.k8s.io/v1 kind: Ingress  metadata:  annotations:  nginx.ingress.kubernetes.io/rewrite-target: /$2  name: ingress-host-bar spec:  ingressClassName: nginx  rules:  - host: \"tomcat.aliyun.com\"  http:  paths:  - pathType: Prefix  path: \"/\"  backend:  service:  name: tomcat-demo  port:  number: 8000  - host: \"nginx.aliyun.com\"  http:  paths:  - pathType: Prefix  path: \"/nginx(/|$)(.*)\"  backend:  service:  name: nginx-demo  port:  number: 8000 流量限制 apiVersion: networking.k8s.io/v1 kind: Ingress metadata:  name: ingress-limit-rate  annotations:  nginx.ingress.kubernetes.io/limit-rps: \"1\" spec:  ingressClassName: nginx  rules:  - host: \"nginx.aliyun.com\"  http:  paths:  - pathType: Exact  path: \"/\"  backend:  service:  name: nginx-demo  port:  number: 8000 存储 PersistentVolume 持久卷（PersistentVolume，PV）将应用需要持久化的数据保存到指定位置。PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS，甚至是本地磁盘。\n简单 YAML 实例\n[root@k8s-master ~]# vim pv.yaml apiVersion: v1 kind: PersistentVolume metadata:  name: host-10m-pv  spec:  storageClassName: host-test  accessModes:  - ReadWriteOnce  capacity:  storage: 10Mi  hostPath:  path: /tmp/host-10m-pv/  # 创建PV [root@k8s-master ~]# kubectl apply -f pv.yaml  persistentvolume/host-10m-pv created [root@k8s-master ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE host-10m-pv 10Mi RWO Retain Available host-test 6s PersistentVolumeClaim 持久卷申领（PersistentVolumeClaim，PVC）申明需要使用的持久卷规格。（例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载。\n ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。 ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。 ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。  简单 YAML 实例\nPVC 的内容与 PV 很像，但它不表示实际的存储，而是一个“申请”或者“声明”，spec 里的字段描述的是对存储的“期望状态”。\n[root@k8s-master ~]# vim pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata:  name: host-5m-pvc  spec:  storageClassName: host-test  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 5Mi # 创建PVC [root@k8s-master ~]# kubectl apply -f pvc.yaml  persistentvolumeclaim/host-5m-pvc created  [root@k8s-master ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE host-5m-pvc Bound host-10m-pv 10Mi RWO host-test 3s  [root@k8s-master ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE host-10m-pv 10Mi RWO Retain Bound default/host-5m-pvc host-test 2m53s 一旦 PVC 对象创建成功，Kubernetes 就会立即通过 StorageClass、resources 等条件在集群里查找符合要求的 PV，如果找到合适的存储对象就会把它俩“绑定”在一起。看到这两个对象的状态都是 Bound，也就是说存储申请成功。\n# 把 PVC 的申请容量改成 100MB [root@k8s-master ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE host-10m-pv 10Mi RWO Retain Available host-test 9s [root@k8s-master ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE host-5m-pvc Pending host-test 7s 看到 PVC 会一直处于 Pending 状态，这意味着 Kubernetes 在系统里没有找到符合要求的存储，无法分配资源，只能等有满足要求的 PV 才能完成绑定。\nNFS 前面使用的是 HostPath，存储卷只能在本机使用。要想让存储卷真正能被 Pod 任意挂载，需要改成网络存储。\n安装NFS\n在所有节点安装\nyum install -y nfs-utils  # 创建网络共享目录 [root@k8s-master ~]# mkdir -p /nfs/data  # 配置 NFS 访问共享目录 [root@k8s-master ~]# echo \"/nfs/data/ *(insecure,rw,sync,no_root_squash)\"  /etc/exports  # 让配置生效，验证效果 [root@k8s-master ~]# exportfs -r [root@k8s-master ~]# exportfs -v /nfs/data (sync,wdelay,hide,no_subtree_check,sec=sys,rw,no_root_squash,no_all_squash)  # 启动 NFS 服务器 systemctl enable rpcbind --now systemctl enable nfs-server --now  # node 节点上查看 [root@k8s-node1 ~]# showmount -e 172.21.252.80 Export list for 172.21.252.80: /nfs/data *  # 创建网络存储挂载点 [root@k8s-node1 ~]# mkdir -p /nfs/data [root@k8s-node2 ~]# mkdir -p /nfs/data  # 把 NFS 服务器的共享目录挂载到/nfs/data [root@k8s-node1 ~]# mount -t nfs 172.21.252.80:/nfs/data /nfs/data [root@k8s-node2 ~]# mount -t nfs 172.21.252.80:/nfs/data /nfs/data  # 测试 [root@k8s-node1 ~]# echo \"nfs server\"  /nfs/data/nfs.txt [root@k8s-master data]# ls nfs.txt 使用NFS静态存储卷 创建PV池\n[root@k8s-master ~]# mkdir -p /nfs/data/{01,02,03} 使用NFS创建PV\n[root@k8s-master data]# vim mult-pv.yaml apiVersion: v1 kind: PersistentVolume metadata:  name: pv01-10m spec:  capacity:  storage: 10M  accessModes:  - ReadWriteMany  storageClassName: nfs  nfs:  path: /nfs/data/01  server: 172.21.252.80 --- apiVersion: v1 kind: PersistentVolume metadata:  name: pv02-1gi spec:  capacity:  storage: 1Gi  accessModes:  - ReadWriteMany  storageClassName: nfs  nfs:  path: /nfs/data/02  server: 172.21.252.80 --- apiVersion: v1 kind: PersistentVolume metadata:  name: pv03-3gi spec:  capacity:  storage: 3Gi  accessModes:  - ReadWriteMany  storageClassName: nfs  nfs:  path: /nfs/data/03  server: 172.21.252.80 使用NFS创建PVC并绑定\n[root@k8s-master data]# cat mult-pvc.yaml  apiVersion: v1 kind: PersistentVolumeClaim metadata:  name: nginx-pvc spec:  accessModes:  - ReadWriteMany  resources:  requests:  storage: 200Mi  storageClassName: nfs 创建Pod绑定PVC\napiVersion: apps/v1 kind: Deployment metadata:  name: nginx-deploy-pvc  labels:  app: nginx-deploy-pvc spec:  replicas: 2  selector:  matchLabels:  app: nginx-deploy-pvc  template:  metadata:  labels:  app: nginx-deploy-pvc  spec:  containers:  - image: nginx  name: nginx  volumeMounts:  - name: html  mountPath: /usr/share/nginx/html  volumes:  - name: html  persistentVolumeClaim:  claimName: nginx-pvc [root@k8s-master ~]# kubectl apply -f mult-pv.yaml persistentvolume/pv01-10m created persistentvolume/pv02-1gi created persistentvolume/pv03-3gi created  [root@k8s-master ~]# kubectl apply -f mult-pvc.yaml persistentvolumeclaim/nginx-pvc created  [root@k8s-master ~]# kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/host-10m-pv 10Mi RWO Retain Available host-test 81m persistentvolume/pv01-10m 10M RWX Retain Available nfs 19s persistentvolume/pv02-1gi 1Gi RWX Retain Bound default/nginx-pvc nfs 19s persistentvolume/pv03-3gi 3Gi RWX Retain Available nfs 19s  NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/host-5m-pvc Pending host-test 81m persistentvolumeclaim/nginx-pvc Bound pv02-1gi 1Gi RWX nfs 15s [root@k8s-master ~]# kubectl apply -f nginx-pvc.yaml deployment.apps/nginx-deploy-pvc created  # 可以看到刚创建的pod和 pv02-1gi 绑定了 [root@k8s-master ~]# kubectl get pv,pvc NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE persistentvolume/host-10m-pv 10Mi RWO Retain Available host-test 82m persistentvolume/pv01-10m 10M RWX Retain Available nfs 41s persistentvolume/pv02-1gi 1Gi RWX Retain Bound default/nginx-pvc nfs 41s persistentvolume/pv03-3gi 3Gi RWX Retain Available nfs 41s  NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE persistentvolumeclaim/host-5m-pvc Pending host-test 81m persistentvolumeclaim/nginx-pvc Bound pv02-1gi 1Gi RWX nfs 37s  # 进入pod新增一个文件，再回到主机的/nfs/data/02目录下查看 [root@k8s-master 02]# kubectl exec -it nginx-deploy-pvc-79fc8558c7-wzzfl -- /bin/bash root@nginx-deploy-pvc-79fc8558c7-wzzfl:/# cd /usr/share/nginx/html/ root@nginx-deploy-pvc-79fc8558c7-wzzfl:/usr/share/nginx/html# ls root@nginx-deploy-pvc-79fc8558c7-wzzfl:/usr/share/nginx/html# echo 111  index.html  [root@k8s-master ~]# cat /nfs/data/02/index.html  111 使用NFS动态存储卷 在一个大集群里，每天可能会有几百几千个应用需要 PV 存储，PV 人工管理不太现实。而且 PV 的大小也很难精确控制，容易出现空间不足或者空间浪费的情况。\n动态存储卷它可以用 StorageClass 绑定一个 Provisioner 对象，而这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。\n目前，Kubernetes 里每类存储设备都有相应的 Provisioner 对象，对于 NFS 来说，它的 Provisioner 就是“NFS subdir external provisioner”，项目地址。\nNFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，在 GitHub 的 deploy 目录里是部署它所需的 YAML 文件，一共有三个，分别是 rbac.yaml、class.yaml 和 deployment.yaml。\n[root@k8s-master ~]# cat class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:  name: nfs-client provisioner: k8s-sigs.io/nfs-subdir-external-provisioner # or choose another name, must match deployment's env PROVISIONER_NAME' parameters:  archiveOnDelete: \"false\"   archiveOnDelete: “false”\t# 自动回收存储空间\nonDelete: “retain”\t#暂时保留分配的存储，之后再手动删除\n 把 namespace: default 全部换成 namespace: kube-system\n[root@k8s-master ~]# sed -i 's/namespace: default/namespace: kube-system/g' rbac.yaml  [root@k8s-master ~]# cat rbac.yaml  apiVersion: v1 kind: ServiceAccount metadata:  name: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: nfs-client-provisioner-runner rules:  - apiGroups: [\"\"]  resources: [\"nodes\"]  verbs: [\"get\", \"list\", \"watch\"]  - apiGroups: [\"\"]  resources: [\"persistentvolumes\"]  verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"]  - apiGroups: [\"\"]  resources: [\"persistentvolumeclaims\"]  verbs: [\"get\", \"list\", \"watch\", \"update\"]  - apiGroups: [\"storage.k8s.io\"]  resources: [\"storageclasses\"]  verbs: [\"get\", \"list\", \"watch\"]  - apiGroups: [\"\"]  resources: [\"events\"]  verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: run-nfs-client-provisioner subjects:  - kind: ServiceAccount  name: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: kube-system roleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: leader-locking-nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: kube-system rules:  - apiGroups: [\"\"]  resources: [\"endpoints\"]  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: leader-locking-nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: kube-system subjects:  - kind: ServiceAccount  name: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: kube-system roleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io  把 namespace: default 改成 namespace: kube-system，修改 volumes 和 env 里的 IP 地址和共享目录名，必须和集群里的 NFS 服务器配置一样。\n[root@k8s-master ~]# cat deployment.yaml  apiVersion: apps/v1 kind: Deployment metadata:  name: nfs-client-provisioner  labels:  app: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: kube-system spec:  replicas: 1  strategy:  type: Recreate  selector:  matchLabels:  app: nfs-client-provisioner  template:  metadata:  labels:  app: nfs-client-provisioner  spec:  serviceAccountName: nfs-client-provisioner  containers:  - name: nfs-client-provisioner  image: chronolaw/nfs-subdir-external-provisioner:v4.0.2  volumeMounts:  - name: nfs-client-root  mountPath: /persistentvolumes  env:  - name: PROVISIONER_NAME  value: k8s-sigs.io/nfs-subdir-external-provisioner  - name: NFS_SERVER  value: 172.21.252.80  - name: NFS_PATH  value: /nfs/data  volumes:  - name: nfs-client-root  nfs:  server: 172.21.252.80  path: /nfs/data 先把前面创建的pod pv pvc删除\n[root@k8s-master ~]# kubectl get pv,pvc No resources found  [root@k8s-master ~]# kubectl apply -f rbac.yaml  serviceaccount/nfs-client-provisioner created clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created  [root@k8s-master ~]# kubectl apply -f class.yaml  storageclass.storage.k8s.io/nfs-client created  [root@k8s-master ~]# kubectl apply -f deployment.yaml deployment.apps/nfs-client-provisioner created  # 看到 NFS Provisioner 已经运行起来了 [root@k8s-master ~]# kubectl get deploy -n kube-system NAME READY UP-TO-DATE AVAILABLE AGE calico-kube-controllers 1/1 1 1 27h coredns 2/2 2 2 27h nfs-client-provisioner 1/1 1 1 106s 定义一个 PVC，向系统申请 500MB 的存储空间，使用的 StorageClass 是默认的 nfs-client\napiVersion: v1 kind: PersistentVolumeClaim metadata:  name: nginx-pvc spec:  accessModes:  - ReadWriteMany  resources:  requests:  storage: 500Mi  storageClassName: nfs-client  [root@k8s-master ~]# kubectl apply -f mult-pvc.yaml persistentvolumeclaim/nginx-pvc created  # 还是用之前的 nginx-pvc.yaml [root@k8s-master ~]# kubectl apply -f nginx-pvc.yaml deployment.apps/nginx-deploy-pvc created  # 可以看到NFS Provisioner 自动创建一个PV，大小刚好是500Mi [root@k8s-master ~]# kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE nginx-pvc Bound pvc-4589a47a-5e10-444b-b813-6a8d66b39283 500Mi RWX nfs-client 35s  [root@k8s-master ~]# kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS REASON AGE pvc-4589a47a-5e10-444b-b813-6a8d66b39283 500Mi RWX Delete Bound default/nginx-pvc nfs-client 33s  # NFS 服务器上查看共享目录会多出一个目录 [root@k8s-master data]# ls default-nginx-pvc-pvc-4589a47a-5e10-444b-b813-6a8d66b39283 配置管理 应用程序有很多类别的配置信息，但从数据安全的角度来看可以分成两类：\n  一类是明文配置，也就是不保密，可以任意查询修改，比如服务端口、运行参数、文件路径等等。\n  另一类则是机密配置，由于涉及敏感信息需要保密，不能随便查看，比如密码、密钥、证书等等。\n  这两类配置信息本质上都是字符串，只是由于安全性的原因，在存放和使用方面有些差异，所以 Kubernetes 也就定义了两个 API 对象，ConfigMap 用来保存明文配置，Secret 用来保存秘密配置。\nConfigMap ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。ConfigMap 将你的环境配置信息和容器镜像解耦，便于应用配置的修改。\n# 用命令 kubectl create 生成 yaml 模板 [root@k8s-master data]# kubectl create cm info --dry-run=client -o yaml apiVersion: v1 kind: ConfigMap metadata:  creationTimestamp: null  name: info ConfigMap 的 YAML 没有 spec 等其他字段，而是用 data 字段\n# 要生成带有 data 字段的 YAML 样板，可以在 kubectl create 后面多加一个参数 --from-literal [root@k8s-master data]# kubectl create cm conf --from-literal=k=v --dry-run=client -o yaml apiVersion: v1 data:  k: v kind: ConfigMap metadata:  creationTimestamp: null  name: info  # 增加一些 Key-Value apiVersion: v1 kind: ConfigMap metadata:  name: info  data:  count: '10'  debug: 'on'  path: '/etc/systemd'  greeting: | say hello to kubernetes.  # 指定配置文件生成 ConfigMap，--from-file= [root@k8s-master ~]# cat redis.conf appendonly yes  [root@k8s-master ~]# kubectl create cm redis.conf --from-file=redis.conf configmap/redis.conf created  [root@k8s-master ~]# kubectl get cm redis.conf -o yaml apiVersion: v1 data:  redis.conf: | appendonly yes kind: ConfigMap metadata:  name: redis.conf  namespace: default  [root@k8s-master ~]# kubectl get cm NAME DATA AGE conf 0 11h kube-root-ca.crt 1 40h redis.conf 1 2m26s # 创建 ConfigMap [root@k8s-master ~]# kubectl apply -f conf.yaml configmap/info created  # 查看 ConfigMap 的状态 [root@k8s-master ~]# kubectl get cm NAME DATA AGE conf 0 11h info 4 4s kube-root-ca.crt 1 40h  [root@k8s-master ~]# kubectl describe cm info Name: info Namespace: default Labels:  Annotations:   Data ==== debug: ---- on greeting: ---- say hello to kubernetes.  path: ---- /etc/systemd count: ---- 10 Events:  现在 ConfigMap 的 Key-Value 信息就已经存入了 etcd 数据库，后续就可以被其他 API 对象使用。\nSecret Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者容器镜像中来说更加安全和灵活。\n假设我们需要从自己搭建的私有镜像仓库拉取镜像，比如 Harbor\nkubectl create secret docker-registry harbor-registry \\ --docker-server= \\ --docker-username= \\ --docker-password= \\ --docker-email= Secret 和 ConfigMap 的结构和用法很类似，不过在 Kubernetes 里 Secret 对象又细分出很多类，比如：\n 访问私有镜像仓库的认证信息 身份识别的凭证信息 HTTPS 通信的证书和私钥 一般的机密信息（格式由用户自行解释）  # 生成 secret YAML 模板文件 [root@k8s-master ~]# kubectl create secret generic user --from-literal=name=root --dry-run=client -o yaml apiVersion: v1 data:  name: cm9vdA==\t# 做了 Base64 编码 kind: Secret metadata:  creationTimestamp: null  name: user  # 增加一些 Key-Value [root@k8s-master ~]# kubectl create secret generic user --from-literal=name=root --from-literal=pwd=123456 --from-literal=db=mysql --dry-run=client -o yaml apiVersion: v1 data:  db: bXlzcWw=\t# mysql  name: cm9vdA==\t# root  pwd: MTIzNDU2\t# 123456 kind: Secret metadata:  creationTimestamp: null  name: user # 创建 Secret [root@k8s-master ~]# kubectl apply -f secret.yaml ksecret/user created  # 查看 Secret [root@k8s-master ~]# kubectl get secret NAME TYPE DATA AGE default-token-dvp24 kubernetes.io/service-account-token 3 40h user Opaque 3 13s  # Data 不能直接看到内容，只能看到数据大小 [root@k8s-master ~]# kubectl describe secret user Name: user Namespace: default Labels:  Annotations:   Type: Opaque  Data ==== db: 5 bytes name: 4 bytes pwd: 6 bytes 使用ConfigMap 和 Secret ConfigMap 和 Secret 只是一些存储在 etcd 里的字符串，所以如果想要在运行时产生效果，就必须要以某种方式“注入”到 Pod 里，让应用去读取。在这方面的处理上 Kubernetes 和 Docker 是一样的，也是两种途径：环境变量和加载文件。\n环境变量 kubectl explain pod.spec.containers.env.valueFrom valueFrom 字段指定了环境变量值的来源，可以是 configMapKeyRef 或者 secretKeyRef ，再进一步指定应用的 ConfigMap/Secret 的 name 和它里面的 key。\n简单示例：\n这里定义了 4 个环境变量，COUNT、GREETING、USERNAME、PASSWORD。\napiVersion: v1 kind: Pod metadata:  name: env-pod  spec:  containers:  - env:  - name: COUNT  valueFrom:  configMapKeyRef:  name: info  key: count  - name: GREETING  valueFrom:  configMapKeyRef:  name: info  key: greeting  - name: USERNAME  valueFrom:  secretKeyRef:  name: user  key: name  - name: PASSWORD  valueFrom:  secretKeyRef:  name: user  key: pwd   image: busybox  name: busy  imagePullPolicy: IfNotPresent  command: [\"/bin/sleep\", \"300\"] # 创建 Pod [root@k8s-master ~]# kubectl apply -f env-pod.yaml  pod/env-pod created  [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE env-pod 1/1 Running 0 2m8s  / # echo $COUNT $GREETING 10 say hello to kubernetes. / # echo $USERNAME $PASSWORD root 123456 加载文件 kubectl explain pod.spec.volumes 在 spec 里增加一个 volumes 字段，然后再定义卷的名字和引用的 ConfigMap/Secret。\n下面定义两个 Volume，分别引用 ConfigMap 和 Secret，名字是 cm-vol 和 sec-vol：\nspec:  volumes:  - name: cm-vol  configMap:  name: info  - name: sec-vol  secret:  secretName: user 把定义好的 Volume 挂载到容器里的某个路径下：\n containers:  - volumeMounts:  - mountPath: /tmp/cm-items  name: cm-vol  - mountPath: /tmp/sec-items  name: sec-vol 定义 Pod YAML 文件\napiVersion: v1 kind: Pod metadata:  name: vol-pod  spec:  volumes:  - name: cm-vol  configMap:  name: info  - name: sec-vol  secret:  secretName: user   containers:  - volumeMounts:  - mountPath: /tmp/cm-items  name: cm-vol  - mountPath: /tmp/sec-items  name: sec-vol   image: busybox  name: busy  imagePullPolicy: IfNotPresent  command: [\"/bin/sleep\", \"1800\"] 创建Pod，然后进Pod查看挂载路径\n[root@k8s-master ~]# kubectl apply -f vol-pod.yaml pod/vol-pod created  [root@k8s-master ~]# kubectl get pod NAME READY STATUS RESTARTS AGE vol-pod 1/1 Running 0 20s  [root@k8s-master ~]# kubectl exec -it vol-pod -- sh / # ls /tmp/cm-items/ count debug greeting path / # ls /tmp/sec-items/ db name pwd / # cat /tmp/cm-items/greeting say hello to kubernetes. / # cat /tmp/sec-items/pwd 123456/ # exit ConfigMap 和 Secret 都变成了目录的形式，而它们里面的 Key-Value 变成了一个个的文件，而文件名就是 Key。以 Volume 的方式来使用 ConfigMap/Secret，就和环境变量不太一样。环境变量用法简单，更适合存放简短的字符串，而 Volume 更适合存放大数据量的配置文件，在 Pod 里加载成文件后让应用直接读取使用。\n修改 ConfigMap/Secret 的 YAML，通过env环境变量的方式加载的配置不会更新，通过volume方式加载的会更新配置。\n[root@k8s-master ~]# kubectl edit cm info apiVersion: v1 kind: ConfigMap metadata:  name: info  data:  count: '1000'  [root@k8s-master ~]# kubectl exec -it vol-pod -- sh / # cat /tmp/cm-items/count  1000/ # exit  如果部署的中间件配置值未更改，是因为Pod部署的中间件自己本身没有热更新能力，需要重新启动 Pod 才能从关联的 ConfigMap 中获取更新的值。\n KubeSphere 持续更新中……\n","wordCount":"5713","inLanguage":"en","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"Me"},"mainEntityOfPage":{"@type":"WebPage","@id":"https://jiayi26.github.io/posts/kubernetes/"},"publisher":{"@type":"Organization","name":"JIAYI's Blog","logo":{"@type":"ImageObject","url":"https://xx.mzqcgc.cn/FrbOmxJvq96XDbDPXbiBWM0LU9e3"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=https://jiayi26.github.io/ accesskey=h title="Home (Alt + H)">Home</a>
<span class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></span></div><ul id=menu><li><a href=https://jiayi26.github.io/categories/ title=Categories><span>Categories</span></a></li><li><a href=https://jiayi26.github.io/tags/ title=Tags><span>Tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=https://jiayi26.github.io/>Home</a>&nbsp;»&nbsp;<a href=https://jiayi26.github.io/posts/>Posts</a></div><h1 class=post-title>Kubernetes</h1><div class=post-meta>27 min&nbsp;·&nbsp;Me&nbsp;|&nbsp;<a href=https://github.com/Pass-JIAYI%27s.github.io/post/posts/Kubernetes.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=post-content><h2 id=kubernetes是什么>Kubernetes是什么？<a hidden class=anchor aria-hidden=true href=#kubernetes是什么>#</a></h2><p>Kubernetes 是一个生产级别的容器编排平台和集群管理系统。用于管理容器化的工作负载和服务，可促进声明式配置和自动化。 Kubernetes 拥有一个庞大且快速增长的生态，其服务、支持和工具的使用范围相当广泛。</p><p><img loading=lazy src=https://d33wubrfki0l68.cloudfront.net/26a177ede4d7b032362289c6fccd448fc4a91174/eb693/images/docs/container_evolution.svg alt=logo></p><h3 id=传统部署时代>传统部署时代<a hidden class=anchor aria-hidden=true href=#传统部署时代>#</a></h3><p>早期，各个组织是在物理服务器上运行应用程序。 由于无法限制在物理服务器中运行的应用程序资源使用，因此会导致资源分配问题。 例如，如果在同一台物理服务器上运行多个应用程序， 则可能会出现一个应用程序占用大部分资源的情况，而导致其他应用程序的性能下降。 一种解决方案是将每个应用程序都运行在不同的物理服务器上， 但是当某个应用程式资源利用率不高时，剩余资源无法被分配给其他应用程式， 而且维护许多物理服务器的成本很高。</p><h3 id=虚拟化部署时代>虚拟化部署时代<a hidden class=anchor aria-hidden=true href=#虚拟化部署时代>#</a></h3><p>因此，虚拟化技术被引入了。虚拟化技术允许你在单个物理服务器的 CPU 上运行多台虚拟机（VM）。 虚拟化能使应用程序在不同 VM 之间被彼此隔离，且能提供一定程度的安全性， 因为一个应用程序的信息不能被另一应用程序随意访问。</p><p>虚拟化技术能够更好地利用物理服务器的资源，并且因为可轻松地添加或更新应用程序， 而因此可以具有更高的可扩缩性，以及降低硬件成本等等的好处。 通过虚拟化，你可以将一组物理资源呈现为可丢弃的虚拟机集群。</p><p>每个 VM 是一台完整的计算机，在虚拟化硬件之上运行所有组件，包括其自己的操作系统。</p><h3 id=容器部署时代>容器部署时代<a hidden class=anchor aria-hidden=true href=#容器部署时代>#</a></h3><p>容器类似于 VM，但是更宽松的隔离特性，使容器之间可以共享操作系统（OS）。 因此，容器比起 VM 被认为是更轻量级的。且与 VM 类似，每个容器都具有自己的文件系统、CPU、内存、进程空间等。 由于它们与基础架构分离，因此可以跨云和 OS 发行版本进行移植。</p><p>容器因具有许多优势而变得流行起来，例如：</p><ul><li>敏捷应用程序的创建和部署：与使用 VM 镜像相比，提高了容器镜像创建的简便性和效率。</li><li>持续开发、集成和部署：通过快速简单的回滚（由于镜像不可变性）， 提供可靠且频繁的容器镜像构建和部署。</li><li>关注开发与运维的分离：在构建、发布时创建应用程序容器镜像，而不是在部署时， 从而将应用程序与基础架构分离。</li><li>可观察性：不仅可以显示 OS 级别的信息和指标，还可以显示应用程序的运行状况和其他指标信号。</li><li>跨开发、测试和生产的环境一致性：在笔记本计算机上也可以和在云中运行一样的应用程序。</li><li>跨云和操作系统发行版本的可移植性：可在 Ubuntu、RHEL、CoreOS、本地、 Google Kubernetes Engine 和其他任何地方运行。</li><li>以应用程序为中心的管理：提高抽象级别，从在虚拟硬件上运行 OS 到使用逻辑资源在 OS 上运行应用程序。</li><li>松散耦合、分布式、弹性、解放的微服务：应用程序被分解成较小的独立部分， 并且可以动态部署和管理 - 而不是在一台大型单机上整体运行。</li><li>资源隔离：可预测的应用程序性能。</li><li>资源利用：高效率和高密度。</li></ul><h3 id=kubernetes的特性>Kubernetes的特性<a hidden class=anchor aria-hidden=true href=#kubernetes的特性>#</a></h3><p>容器是打包和运行应用程序的好方式。在生产环境中， 你需要管理运行着应用程序的容器，并确保服务不会下线。 例如，如果一个容器发生故障，则你需要启动另一个容器。 如果此行为交由给系统处理，是不是会更容易一些？</p><p>这就是 Kubernetes 要来做的事情！ Kubernetes 为你提供了一个可弹性运行分布式系统的框架。 Kubernetes 会满足你的扩展要求、故障转移你的应用、提供部署模式等。 例如，Kubernetes 可以轻松管理系统的 Canary 部署。</p><ul><li><p><strong>服务发现和负载均衡</strong>
Kubernetes 可以使用 DNS 名称或自己的 IP 地址来曝露容器。 如果进入容器的流量很大， Kubernetes 可以负载均衡并分配网络流量，从而使部署稳定。</p></li><li><p><strong>存储编排</strong>
Kubernetes 允许你自动挂载你选择的存储系统，例如本地存储、公共云提供商等。</p></li><li><p><strong>自动部署和回滚</strong>
你可以使用 Kubernetes 描述已部署容器的所需状态， 它可以以受控的速率将实际状态更改为期望状态。 例如，你可以自动化 Kubernetes 来为你的部署创建新容器， 删除现有容器并将它们的所有资源用于新容器。</p></li><li><p><strong>自动完成装箱计算</strong>
你为 Kubernetes 提供许多节点组成的集群，在这个集群上运行容器化的任务。 你告诉 Kubernetes 每个容器需要多少 CPU 和内存 (RAM)。 Kubernetes 可以将这些容器按实际情况调度到你的节点上，以最佳方式利用你的资源。</p></li><li><p><strong>自我修复</strong>
Kubernetes 将重新启动失败的容器、替换容器、杀死不响应用户定义的运行状况检查的容器， 并且在准备好服务之前不将其通告给客户端。</p></li><li><p><strong>密钥与配置管理</strong>
Kubernetes 允许你存储和管理敏感信息，例如密码、OAuth 令牌和 ssh 密钥。 你可以在不重建容器镜像的情况下部署和更新密钥和应用程序配置，也无需在堆栈配置中暴露密钥。</p></li></ul><h2 id=kubernetes-的基本架构>Kubernetes 的基本架构<a hidden class=anchor aria-hidden=true href=#kubernetes-的基本架构>#</a></h2><p>Kubernetes 采用了现今流行的“<strong>控制面 / 数据面</strong>”（Control Plane / Data Plane）架构，集群里的计算机被称为“节点”（Node），可以是实机也可以是虚机，少量的节点用作控制面来执行集群的管理维护工作，其他的大部分节点都被划归数据面，用来跑业务应用。</p><p>控制面的节点在 Kubernetes 里叫做 Master Node，一般简称为 Master，它是整个集群里最重要的部分，可以说是 Kubernetes 的大脑和心脏。</p><p>数据面的节点叫做 Worker Node，一般就简称为 Worker 或者 Node，相当于 Kubernetes 的手和脚，在 Master 的指挥下干活。</p><p>Node 的数量非常多，构成了一个资源池，Kubernetes 就在这个池里分配资源，调度应用。因为资源被“池化”了，所以管理也就变得比较简单，可以在集群中任意添加或者删除节点。
<img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/components-of-kubernetes.svg alt=logo></p><h3 id=master组件>Master组件<a hidden class=anchor aria-hidden=true href=#master组件>#</a></h3><p>Master组件为集群做出全局决策，比如资源的调度。 以及检测和响应集群事件，例如当不满足部署的 replicas 字段时， 要启动新的 pod）。</p><p>控制平面组件可以在集群中的任何节点上运行。 然而，为了简单起见，设置脚本通常会在同一个计算机上启动所有控制平面组件， 并且不会在此计算机上运行用户容器。</p><p><strong>apiserver</strong> 是整个 Kubernetes 系统的唯一入口，它对外公开了一系列的 RESTful API，并且加上了验证、授权等功能，所有其他组件都只能和它直接通信，可以说是 Kubernetes 里的联络员。</p><p><strong>etcd</strong> 是一个高可用的分布式 Key-Value 数据库，用来持久化存储系统里的各种资源对象和状态，相当于 Kubernetes 里的配置管理员。注意它只与 apiserver 有直接联系，也就是说任何其他组件想要读写 etcd 里的数据都必须经过 apiserver。</p><p><strong>scheduler</strong> 负责容器的编排工作，检查节点的资源状态，把 Pod 调度到最适合的节点上运行，相当于部署人员。因为节点状态和 Pod 信息都存储在 etcd 里，所以 scheduler 必须通过 apiserver 才能获得。</p><p><strong>controller-manager</strong> 负责运行控制器进程。从逻辑上讲， 每个控制器都是一个单独的进程， 但是为了降低复杂性，它们都被编译到同一个可执行文件，并在同一个进程中运行。
负责维护容器和节点等资源的状态，实现故障检测、服务迁移、应用伸缩等功能，相当于监控运维人员。同样地，它也必须通过 apiserver 获得存储在 etcd 里的信息，才能够实现对资源的各种操作。
这些控制器包括：</p><ul><li>节点控制器（Node Controller）：负责在节点出现故障时进行通知和响应</li><li>任务控制器（Job Controller）：监测代表一次性任务的 Job 对象，然后创建 Pods 来运行这些任务直至完成</li><li>端点控制器（Endpoints Controller）：填充端点（Endpoints）对象（即加入 Service 与 Pod）</li><li>服务帐户和令牌控制器（Service Account & Token Controllers）：为新的命名空间创建默认帐户和 API 访问令牌</li></ul><h3 id=node-组件>Node 组件<a hidden class=anchor aria-hidden=true href=#node-组件>#</a></h3><p>Node组件会在每个节点上运行，负责维护运行的 Pod 并提供 Kubernetes 运行环境。</p><p><strong>kubelet</strong> 是 Node 的代理，负责管理 Node 相关的绝大部分操作，Node 上只有它能够与 apiserver 通信，实现状态报告、命令下发、启停容器等功能，相当于是 Node 上的一个“小管家”。</p><p><strong>kube-proxy</strong> 的作用有点特别，它是 Node 的网络代理，只负责管理容器的网络通信，简单来说就是为 Pod 转发 TCP/UDP 数据包，相当于是专职的“小邮差”。</p><p><strong>container-runtime</strong> 我们就比较熟悉了，它是容器和镜像的实际使用者，在 kubelet 的指挥下创建容器，管理 Pod 的生命周期，是真正干活的“苦力”。</p><h3 id=插件addons>插件（Addons）<a hidden class=anchor aria-hidden=true href=#插件addons>#</a></h3><p>插件使用 Kubernetes 资源（DaemonSet、 Deployment 等）实现集群功能。 因为这些插件提供集群级别的功能，插件中命名空间域的资源属于 kube-system 命名空间。</p><p><strong>DNS</strong></p><p>DNS在 Kubernetes 集群里实现了域名解析服务，能够让我们以域名而不是 IP 地址的方式来互相通信，是服务发现和负载均衡的基础。由于它对微服务、服务网格等架构至关重要，所以基本上是 Kubernetes 的必备插件。</p><p><strong>Dashboard</strong></p><p>Dashboard 就是仪表盘。是 Kubernetes 集群的通用的、基于 Web 的用户界面。 它使用户可以管理集群中运行的应用程序以及集群本身， 并进行故障排除。</p><h2 id=kubernetes集群搭建>Kubernetes集群搭建<a hidden class=anchor aria-hidden=true href=#kubernetes集群搭建>#</a></h2><h3 id=安装kubeadm>安装kubeadm<a hidden class=anchor aria-hidden=true href=#安装kubeadm>#</a></h3><ul><li>一台兼容的 Linux 主机。Kubernetes 项目为基于 Debian 和 Red Hat 的 Linux 发行版以及一些不提供包管理器的发行版提供通用的指令。</li><li>每台机器 2 GB 或更多的 RAM（如果少于这个数字将会影响你应用的运行内存）。</li><li>CPU 2 核心及以上。</li><li>集群中的所有机器的网络彼此均能相互连接（公网和内网都可以）。</li><li>节点之中不可以有重复的主机名、MAC 地址或 product_uuid。请参见<a href=https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#verify-mac-address>这里</a>了解更多详细信息。</li><li>开启机器上的某些端口。请参见<a href=https://kubernetes.io/zh-cn/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports>这里</a>了解更多详细信息。</li><li>禁用交换分区。为了保证 kubelet 正常工作，你<strong>必须</strong>禁用交换分区。</li></ul><h4 id=基础环境>基础环境<a hidden class=anchor aria-hidden=true href=#基础环境>#</a></h4><p>这里准备了三台阿里云主机，在三台主机上执行以下操作</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 安装Docker</span>
</span></span><span style=display:flex><span>yum install -y yum-utils
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>yum-config-manager <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--add-repo <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>yum install -y docker-ce-20.10.7 docker-ce-cli-20.10.7 containerd.io-1.4.6
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl enable docker --now
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 配置镜像加速</span>
</span></span><span style=display:flex><span>tee /etc/docker/daemon.json <span style=color:#e6db74>&lt;&lt;-&#39;EOF&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>{
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;registry-mirrors&#34;: [&#34;https://5wr347bg.mirror.aliyuncs.com&#34;],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;exec-opts&#34;: [&#34;native.cgroupdriver=systemd&#34;],
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;log-driver&#34;: &#34;json-file&#34;,
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;log-opts&#34;: {
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    &#34;max-size&#34;: &#34;100m&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  },
</span></span></span><span style=display:flex><span><span style=color:#e6db74>  &#34;storage-driver&#34;: &#34;overlay2&#34;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>}
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl daemon-reload
</span></span><span style=display:flex><span>systemctl restart docker
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 设置hostname</span>
</span></span><span style=display:flex><span>hostnamectl set-hostname xxx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 将 SELinux 设置为 permissive 模式（相当于将其禁用）</span>
</span></span><span style=display:flex><span>setenforce <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>sed -i <span style=color:#e6db74>&#39;s/^SELINUX=enforcing$/SELINUX=permissive/&#39;</span> /etc/selinux/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 关闭swap</span>
</span></span><span style=display:flex><span>swapoff -a  
</span></span><span style=display:flex><span>sed -ri <span style=color:#e6db74>&#39;s/.*swap.*/#&amp;/&#39;</span> /etc/fstab
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 转发 IPv4 并让 iptables 看到桥接流量</span>
</span></span><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt;EOF | sudo tee /etc/modules-load.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#e6db74>br_netfilter
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 设置所需的 sysctl 参数，参数在重新启动后保持不变</span>
</span></span><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt;EOF | sudo tee /etc/sysctl.d/k8s.conf
</span></span></span><span style=display:flex><span><span style=color:#e6db74>net.bridge.bridge-nf-call-iptables  = 1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>net.bridge.bridge-nf-call-ip6tables = 1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 应用 sysctl 参数而不重新启动</span>
</span></span><span style=display:flex><span>sysctl --system
</span></span></code></pre></div><h4 id=安装-kubeadmkubelet-和-kubectl>安装 kubeadm、kubelet 和 kubectl<a hidden class=anchor aria-hidden=true href=#安装-kubeadmkubelet-和-kubectl>#</a></h4><ul><li><code>kubeadm</code>：用来初始化集群的指令。</li><li><code>kubelet</code>：在集群中的每个节点上用来启动 Pod 和容器等。</li><li><code>kubectl</code>：用来与集群通信的命令行工具。</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>cat <span style=color:#e6db74>&lt;&lt;EOF | sudo tee /etc/yum.repos.d/kubernetes.repo
</span></span></span><span style=display:flex><span><span style=color:#e6db74>[kubernetes]
</span></span></span><span style=display:flex><span><span style=color:#e6db74>name=Kubernetes
</span></span></span><span style=display:flex><span><span style=color:#e6db74>baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64
</span></span></span><span style=display:flex><span><span style=color:#e6db74>enabled=1
</span></span></span><span style=display:flex><span><span style=color:#e6db74>gpgcheck=0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>repo_gpgcheck=0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#e6db74>   http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg
</span></span></span><span style=display:flex><span><span style=color:#e6db74>exclude=kubelet kubeadm kubectl
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>yum install -y kubelet-1.20.9 kubeadm-1.20.9 kubectl-1.20.9 --disableexcludes<span style=color:#f92672>=</span>kubernetes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>systemctl enable --now kubelet
</span></span></code></pre></div><h3 id=使用kubeadm引导集群>使用kubeadm引导集群<a hidden class=anchor aria-hidden=true href=#使用kubeadm引导集群>#</a></h3><h4 id=下载各个机器需要的镜像>下载各个机器需要的镜像<a hidden class=anchor aria-hidden=true href=#下载各个机器需要的镜像>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>tee ./images.sh <span style=color:#e6db74>&lt;&lt;-&#39;EOF&#39;
</span></span></span><span style=display:flex><span><span style=color:#e6db74>#!/bin/bash
</span></span></span><span style=display:flex><span><span style=color:#e6db74>images=(
</span></span></span><span style=display:flex><span><span style=color:#e6db74>kube-apiserver:v1.20.9
</span></span></span><span style=display:flex><span><span style=color:#e6db74>kube-proxy:v1.20.9
</span></span></span><span style=display:flex><span><span style=color:#e6db74>kube-controller-manager:v1.20.9
</span></span></span><span style=display:flex><span><span style=color:#e6db74>kube-scheduler:v1.20.9
</span></span></span><span style=display:flex><span><span style=color:#e6db74>coredns:1.7.0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>etcd:3.4.13-0
</span></span></span><span style=display:flex><span><span style=color:#e6db74>pause:3.2
</span></span></span><span style=display:flex><span><span style=color:#e6db74>)
</span></span></span><span style=display:flex><span><span style=color:#e6db74>for imageName in ${images[@]} ; do
</span></span></span><span style=display:flex><span><span style=color:#e6db74>docker pull registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images/$imageName
</span></span></span><span style=display:flex><span><span style=color:#e6db74>done
</span></span></span><span style=display:flex><span><span style=color:#e6db74>EOF</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>chmod +x ./images.sh <span style=color:#f92672>&amp;&amp;</span> ./images.sh
</span></span></code></pre></div><h4 id=初始化主节点>初始化主节点<a hidden class=anchor aria-hidden=true href=#初始化主节点>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e>#所有机器添加master域名映射</span>
</span></span><span style=display:flex><span>echo <span style=color:#e6db74>&#34;172.21.252.80  cluster-endpoint&#34;</span> &gt;&gt; /etc/hosts
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#主节点初始化</span>
</span></span><span style=display:flex><span>kubeadm init <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--apiserver-advertise-address<span style=color:#f92672>=</span>172.21.252.80 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--control-plane-endpoint<span style=color:#f92672>=</span>cluster-endpoint <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--image-repository registry.cn-hangzhou.aliyuncs.com/lfy_k8s_images <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--kubernetes-version v1.20.9 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--service-cidr<span style=color:#f92672>=</span>10.96.0.0/16 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--pod-network-cidr<span style=color:#f92672>=</span>192.168.0.0/16
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 看到这个说明初始化成功了</span>
</span></span><span style=display:flex><span>Your Kubernetes control-plane has initialized successfully!
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>To start using your cluster, you need to run the following as a regular user:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  mkdir -p $HOME/.kube
</span></span><span style=display:flex><span>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style=display:flex><span>  sudo chown <span style=color:#66d9ef>$(</span>id -u<span style=color:#66d9ef>)</span>:<span style=color:#66d9ef>$(</span>id -g<span style=color:#66d9ef>)</span> $HOME/.kube/config
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Alternatively, <span style=color:#66d9ef>if</span> you are the root user, you can run:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  export KUBECONFIG<span style=color:#f92672>=</span>/etc/kubernetes/admin.conf
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>You should now deploy a pod network to the cluster.
</span></span><span style=display:flex><span>Run <span style=color:#e6db74>&#34;kubectl apply -f [podnetwork].yaml&#34;</span> with one of the options listed at:
</span></span><span style=display:flex><span>  https://kubernetes.io/docs/concepts/cluster-administration/addons/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>You can now join any number of control-plane nodes by copying certificate authorities
</span></span><span style=display:flex><span>and service account keys on each node and <span style=color:#66d9ef>then</span> running the following as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  kubeadm join cluster-endpoint:6443 --token 6fuiem.x02y9reea0zn32yw <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --discovery-token-ca-cert-hash sha256:a2dd77dfaf63e2f0554bac26454e5545f89a65a5427d315a2640f3df38a03558 <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --control-plane 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Then you can join any number of worker nodes by running the following on each as root:
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>kubeadm join cluster-endpoint:6443 --token 6fuiem.x02y9reea0zn32yw <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>    --discovery-token-ca-cert-hash sha256:a2dd77dfaf63e2f0554bac26454e5545f89a65a5427d315a2640f3df38a03558
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#设置 .kube/config</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># mkdir -p $HOME/.kube</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># sudo chown $(id -u):$(id -g) $HOME/.kube/config</span>
</span></span></code></pre></div><h4 id=安装网络插件>安装网络插件<a hidden class=anchor aria-hidden=true href=#安装网络插件>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl https://docs.projectcalico.org/archive/v3.21/manifests/calico.yaml -O</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f calico.yaml</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看集群部署的应用</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -A</span>
</span></span><span style=display:flex><span>NAMESPACE         NAME                                       READY   STATUS             RESTARTS   AGE
</span></span><span style=display:flex><span>kube-system       calico-kube-controllers-5bb48c55fd-42txw   1/1     Running            <span style=color:#ae81ff>0</span>          3m4s
</span></span><span style=display:flex><span>kube-system       calico-node-l98kx                          1/1     Running            <span style=color:#ae81ff>0</span>          3m11s
</span></span><span style=display:flex><span>kube-system       coredns-5897cd56c4-h8qkb                   1/1     Running            <span style=color:#ae81ff>0</span>          45m
</span></span><span style=display:flex><span>kube-system       coredns-5897cd56c4-xm8hv                   1/1     Running            <span style=color:#ae81ff>0</span>          45m
</span></span><span style=display:flex><span>kube-system       etcd-k8s-master                            1/1     Running            <span style=color:#ae81ff>0</span>          46m
</span></span><span style=display:flex><span>kube-system       kube-apiserver-k8s-master                  1/1     Running            <span style=color:#ae81ff>0</span>          46m
</span></span><span style=display:flex><span>kube-system       kube-controller-manager-k8s-master         1/1     Running            <span style=color:#ae81ff>0</span>          46m
</span></span><span style=display:flex><span>kube-system       kube-proxy-df5n2                           1/1     Running            <span style=color:#ae81ff>0</span>          45m
</span></span><span style=display:flex><span>kube-system       kube-scheduler-k8s-master                  1/1     Running            <span style=color:#ae81ff>0</span>          46m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看集群所有节点</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get nodes</span>
</span></span><span style=display:flex><span>NAME         STATUS   ROLES                  AGE   VERSION
</span></span><span style=display:flex><span>k8s-master   Ready    control-plane,master   47m   v1.20.9
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># kubectl命令</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl --help</span>
</span></span><span style=display:flex><span>kubectl controls the Kubernetes cluster manager.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span> Find more information at: https://kubernetes.io/docs/reference/kubectl/overview/
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Basic Commands <span style=color:#f92672>(</span>Beginner<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>  create        Create a resource from a file or from stdin.
</span></span><span style=display:flex><span>  expose        Take a replication controller, service, deployment or pod and expose it as a new Kubernetes Service
</span></span><span style=display:flex><span>  run           Run a particular image on the cluster
</span></span><span style=display:flex><span>  set           Set specific features on objects
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Basic Commands <span style=color:#f92672>(</span>Intermediate<span style=color:#f92672>)</span>:
</span></span><span style=display:flex><span>  explain       Documentation of resources
</span></span><span style=display:flex><span>  get           Display one or many resources
</span></span><span style=display:flex><span>  edit          Edit a resource on the server
</span></span><span style=display:flex><span>  delete        Delete resources by filenames, stdin, resources and names, or by resources and label selector
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Deploy Commands:
</span></span><span style=display:flex><span>  rollout       Manage the rollout of a resource
</span></span><span style=display:flex><span>  scale         Set a new size <span style=color:#66d9ef>for</span> a Deployment, ReplicaSet or Replication Controller
</span></span><span style=display:flex><span>  autoscale     Auto-scale a Deployment, ReplicaSet, or ReplicationController
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Cluster Management Commands:
</span></span><span style=display:flex><span>  certificate   Modify certificate resources.
</span></span><span style=display:flex><span>  cluster-info  Display cluster info
</span></span><span style=display:flex><span>  top           Display Resource <span style=color:#f92672>(</span>CPU/Memory/Storage<span style=color:#f92672>)</span> usage.
</span></span><span style=display:flex><span>  cordon        Mark node as unschedulable
</span></span><span style=display:flex><span>  uncordon      Mark node as schedulable
</span></span><span style=display:flex><span>  drain         Drain node in preparation <span style=color:#66d9ef>for</span> maintenance
</span></span><span style=display:flex><span>  taint         Update the taints on one or more nodes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Troubleshooting and Debugging Commands:
</span></span><span style=display:flex><span>  describe      Show details of a specific resource or group of resources
</span></span><span style=display:flex><span>  logs          Print the logs <span style=color:#66d9ef>for</span> a container in a pod
</span></span><span style=display:flex><span>  attach        Attach to a running container
</span></span><span style=display:flex><span>  exec          Execute a command in a container
</span></span><span style=display:flex><span>  port-forward  Forward one or more local ports to a pod
</span></span><span style=display:flex><span>  proxy         Run a proxy to the Kubernetes API server
</span></span><span style=display:flex><span>  cp            Copy files and directories to and from containers.
</span></span><span style=display:flex><span>  auth          Inspect authorization
</span></span><span style=display:flex><span>  debug         Create debugging sessions <span style=color:#66d9ef>for</span> troubleshooting workloads and nodes
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Advanced Commands:
</span></span><span style=display:flex><span>  diff          Diff live version against would-be applied version
</span></span><span style=display:flex><span>  apply         Apply a configuration to a resource by filename or stdin
</span></span><span style=display:flex><span>  patch         Update field<span style=color:#f92672>(</span>s<span style=color:#f92672>)</span> of a resource
</span></span><span style=display:flex><span>  replace       Replace a resource by filename or stdin
</span></span><span style=display:flex><span>  wait          Experimental: Wait <span style=color:#66d9ef>for</span> a specific condition on one or many resources.
</span></span><span style=display:flex><span>  kustomize     Build a kustomization target from a directory or a remote url.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Settings Commands:
</span></span><span style=display:flex><span>  label         Update the labels on a resource
</span></span><span style=display:flex><span>  annotate      Update the annotations on a resource
</span></span><span style=display:flex><span>  completion    Output shell completion code <span style=color:#66d9ef>for</span> the specified shell <span style=color:#f92672>(</span>bash or zsh<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Other Commands:
</span></span><span style=display:flex><span>  api-resources Print the supported API resources on the server
</span></span><span style=display:flex><span>  api-versions  Print the supported API versions on the server, in the form of <span style=color:#e6db74>&#34;group/version&#34;</span>
</span></span><span style=display:flex><span>  config        Modify kubeconfig files
</span></span><span style=display:flex><span>  plugin        Provides utilities <span style=color:#66d9ef>for</span> interacting with plugins.
</span></span><span style=display:flex><span>  version       Print the client and server version information
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Usage:
</span></span><span style=display:flex><span>  kubectl <span style=color:#f92672>[</span>flags<span style=color:#f92672>]</span> <span style=color:#f92672>[</span>options<span style=color:#f92672>]</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Use <span style=color:#e6db74>&#34;kubectl &lt;command&gt; --help&#34;</span> <span style=color:#66d9ef>for</span> more information about a given command.
</span></span><span style=display:flex><span>Use <span style=color:#e6db74>&#34;kubectl options&#34;</span> <span style=color:#66d9ef>for</span> a list of global command-line options <span style=color:#f92672>(</span>applies to all commands<span style=color:#f92672>)</span>.
</span></span></code></pre></div><blockquote><p>安装网络插件的时候如果报错，说明版本不兼容，可去<a href=https://docs.tigera.io/calico/latest/getting-started/kubernetes/self-managed-onprem/onpremises#install-calico-with-kubernetes-api-datastore-more-than-50-nodes>calico官网</a>查看对应版本。 error: unable to recognize &ldquo;calico.yaml&rdquo;: no matches for kind &ldquo;PodDisruptionBudget&rdquo; in version &ldquo;policy/v1&rdquo;</p></blockquote><h4 id=加入node节点>加入node节点<a hidden class=anchor aria-hidden=true href=#加入node节点>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node1 ~<span style=color:#f92672>]</span><span style=color:#75715e># kubeadm join cluster-endpoint:6443 --token tevdqf.0dqqjpzye7gtnyj1 \</span>
</span></span><span style=display:flex><span>&gt;     --discovery-token-ca-cert-hash sha256:d1f7101180a99cfb7991c1a972e43f8978b3811afc16033907f45cb40c3bdba2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node2 ~<span style=color:#f92672>]</span><span style=color:#75715e># kubeadm join cluster-endpoint:6443 --token tevdqf.0dqqjpzye7gtnyj1 \</span>
</span></span><span style=display:flex><span>&gt;     --discovery-token-ca-cert-hash sha256:d1f7101180a99cfb7991c1a972e43f8978b3811afc16033907f45cb40c3bdba2
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 再次查看集群所有节点</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get nodes</span>
</span></span><span style=display:flex><span>NAME         STATUS   ROLES                  AGE   VERSION
</span></span><span style=display:flex><span>k8s-master   Ready    control-plane,master   53m   v1.20.9
</span></span><span style=display:flex><span>k8s-node1    Ready    &lt;none&gt;                 94s   v1.20.9
</span></span><span style=display:flex><span>k8s-node2    Ready    &lt;none&gt;                 79s   v1.20.9
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看名称空间</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get ns</span>
</span></span><span style=display:flex><span>NAME                   STATUS   AGE
</span></span><span style=display:flex><span>default                Active   56m
</span></span><span style=display:flex><span>kube-node-lease        Active   57m
</span></span><span style=display:flex><span>kube-public            Active   57m
</span></span><span style=display:flex><span>kube-system            Active   57m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 默认情况下，令牌会在 24 小时后过期。如果要在当前令牌过期后将节点加入集群， 则可以通过在控制平面节点上运行以下命令来创建新令牌</span>
</span></span><span style=display:flex><span>kubeadm token create --print-join-command
</span></span></code></pre></div><h4 id=部署dashboard>部署dashboard<a hidden class=anchor aria-hidden=true href=#部署dashboard>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommended.yaml</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 设置访问端口，把 type: ClusterIP 改为 type: NodePort</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl edit svc kubernetes-dashboard -n kubernetes-dashboard</span>
</span></span><span style=display:flex><span>service/kubernetes-dashboard edited
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看暴露的端口</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get svc -A |grep kubernetes-dashboard</span>
</span></span><span style=display:flex><span>kubernetes-dashboard   dashboard-metrics-scraper   ClusterIP   10.96.188.220   &lt;none&gt;        8000/TCP                 85s
</span></span><span style=display:flex><span>kubernetes-dashboard   kubernetes-dashboard        NodePort    10.96.164.146   &lt;none&gt;        443:32747/TCP            85s
</span></span></code></pre></div><p>设置阿里云安全组规则，浏览器随便访问任一节点的https://ip:port</p><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/image-20230325164643460.png alt=image-20230325164643460></p><p>创建访问账号，准备一个yaml文件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# vi dash.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>admin-user</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kubernetes-dashboard</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRoleBinding</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>admin-user</span>
</span></span><span style=display:flex><span><span style=color:#f92672>roleRef</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>apiGroup</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRole</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cluster-admin</span>
</span></span><span style=display:flex><span><span style=color:#f92672>subjects</span>:
</span></span><span style=display:flex><span>- <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>admin-user</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kubernetes-dashboard</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# kubectl apply -f dash.yaml</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>serviceaccount/admin-user created</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>clusterrolebinding.rbac.authorization.k8s.io/admin-user created</span>
</span></span></code></pre></div><p>获取访问令牌，登录dashboard</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl -n kubernetes-dashboard get secret $(kubectl -n kubernetes-dashboard get sa/admin-user -o jsonpath=&#34;{.secrets[0].name}&#34;) -o go-template=&#34;{{.data.token | base64decode}}&#34;</span>
</span></span><span style=display:flex><span>eyJhbGciOiJSUzI1NiIsImtpZCI6Ik4yNTdoUzBBWXp4TEFLOVl2R0d3em1udm9tZWR2LTVKLXZLZUpDYXRIU0kifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi11c2VyLXRva2VuLW0yZGYyIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluLXVzZXIiLCJrdWJlcm5ldGVzLmlvL3NlcnZpY2VhY2NvdW50L3NlcnZpY2UtYWNjb3VudC51aWQiOiJhMjQ0ODY5Ny00NTA4LTQ0Y2YtYTIyMi0wYzMzYTAwNzk2ZWIiLCJzdWIiOiJzeXN0ZW06c2VydmljZWFjY291bnQ6a3ViZXJuZXRlcy1kYXNoYm9hcmQ6YWRtaW4tdXNlciJ9.vyC-hgoTrEAu7slM1CtKTZ0j9YZ5_say_ENl1vLTDwyvSMgKcZSbyAzVp7FCb8lSDKcFl6Wy6oVdRKeWwE0i3vJgb0t6LKss-2CqdQXmlZ-zGdK4XFjH4GiIhpSvFOQpYyeqiJQmIB8Hk_Lo0QFZ-xr-y1hVeM0-xxXMnxAznuNrz3nZvn3DaKk0kYwLj8iF9mt0RvNg323mD1J6xo8wtQsj2_oZthcy4cqyv_7bkRzBosN5Xz9GSs9tWqMf8DHOQSrufMh7e64Pf2msP-iwiCAdHcsGtT0FAKp1oGGE14_FAD9K2TCJ4oDOWQlHabckXzrlUJRJshBtjDefYxgvwg
</span></span></code></pre></div><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/image-20230325170613586.png alt=image-20230325170613586></p><h2 id=工作负载资源>工作负载资源<a hidden class=anchor aria-hidden=true href=#工作负载资源>#</a></h2><h3 id=pod>Pod<a hidden class=anchor aria-hidden=true href=#pod>#</a></h3><h4 id=什么是pod>什么是Pod<a hidden class=anchor aria-hidden=true href=#什么是pod>#</a></h4><p>Pod 是可以在 Kubernetes 中创建和管理的、最小的可部署的计算单元。</p><p>Pod（就像在鲸鱼荚或者豌豆荚中）是一组（一个或多个）容器； 这些容器共享存储、网络、以及怎样运行这些容器的声明。 Pod 中的内容总是并置（colocated）的并且一同调度，在共享的上下文中运行。</p><p>Pod 为其成员容器提供了两种共享资源：网络和存储。</p><p>Pod 通常不是直接创建的，而是使用工作负载资源创建。如：<code>Deployments</code>，<code>StatefulSet</code>，<code>DaemonSet</code>，<code>CornJob</code>，<code>Job</code>等。</p><h4 id=使用pod>使用Pod<a hidden class=anchor aria-hidden=true href=#使用pod>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 命令行的方式运行</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl run nginx --image=nginx</span>
</span></span><span style=display:flex><span>pod/nginx created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># -w 可以查看启动过程</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -w</span>
</span></span><span style=display:flex><span>NAME    READY   STATUS              RESTARTS   AGE
</span></span><span style=display:flex><span>nginx   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          16s
</span></span><span style=display:flex><span>nginx   1/1     Running             <span style=color:#ae81ff>0</span>          25s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># yaml 文件方式运行</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># vim nginx.yaml</span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Pod
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  containers:
</span></span><span style=display:flex><span>  - name: nginx
</span></span><span style=display:flex><span>    image: nginx:1.14.2
</span></span><span style=display:flex><span>    ports:
</span></span><span style=display:flex><span>    - containerPort: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f nginx.yaml</span>
</span></span><span style=display:flex><span>pod/nginx created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># kubectl get pod 默认查看的是 default 名称空间的 Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME    READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx   1/1     Running   <span style=color:#ae81ff>0</span>          74s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看 Pod 的详细状态，可以看到分配给了node1节点去执行</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl describe pod nginx</span>
</span></span><span style=display:flex><span>Events:
</span></span><span style=display:flex><span>  Type    Reason     Age    From               Message
</span></span><span style=display:flex><span>  ----    ------     ----   ----               -------
</span></span><span style=display:flex><span>  Normal  Scheduled  6m16s  default-scheduler  Successfully assigned default/nginx to k8s-node1
</span></span><span style=display:flex><span>  Normal  Pulled     6m15s  kubelet            Container image <span style=color:#e6db74>&#34;nginx:1.14.2&#34;</span> already present on machine
</span></span><span style=display:flex><span>  Normal  Created    6m15s  kubelet            Created container nginx
</span></span><span style=display:flex><span>  Normal  Started    6m14s  kubelet            Started container nginx
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看 Pod 的运行日志</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl logs nginx</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看更详细信息，每个Pod - k8s都会分配一个ip</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -owide</span>
</span></span><span style=display:flex><span>NAME    READY   STATUS    RESTARTS   AGE    IP              NODE        NOMINATED NODE   READINESS GATES
</span></span><span style=display:flex><span>nginx   1/1     Running   <span style=color:#ae81ff>0</span>          108s   192.168.36.68   k8s-node1   &lt;none&gt;    &lt;none&gt;
</span></span></code></pre></div><blockquote><p>此时的应用还不能外部访问</p></blockquote><h3 id=deployment无状态应用>Deployment（无状态应用）<a hidden class=anchor aria-hidden=true href=#deployment无状态应用>#</a></h3><p>专门用来部署应用程序的，能够让应用永不宕机，多用来发布无状态的应用，是 Kubernetes 里最常用也是最有用的一个对象。控制Pod，使Pod拥有多副本，自愈，扩缩容等能力。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 用run启一个pod </span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl run nginx --image=nginx</span>
</span></span><span style=display:flex><span>pod/nginx created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 用 create deployment 启一个pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl create deployment tomcat --image=tomcat</span>
</span></span><span style=display:flex><span>deployment.apps/tomcat created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx                     1/1     Running   <span style=color:#ae81ff>0</span>          2m24s
</span></span><span style=display:flex><span>tomcat-7d987c7694-gwmjr   1/1     Running   <span style=color:#ae81ff>0</span>          102s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># nginx 删了就没了，tomcat 删了后会重新启动一个tomcat</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl delete pod nginx</span>
</span></span><span style=display:flex><span>pod <span style=color:#e6db74>&#34;nginx&#34;</span> deleted
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>tomcat-7d987c7694-gwmjr   1/1     Running   <span style=color:#ae81ff>0</span>          2m24s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># deployment 的自愈能力</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl delete pod tomcat-7d987c7694-gwmjr</span>
</span></span><span style=display:flex><span>pod <span style=color:#e6db74>&#34;tomcat-7d987c7694-gwmjr&#34;</span> deleted
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>tomcat-7d987c7694-78b2d   1/1     Running   <span style=color:#ae81ff>0</span>          42s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 删除</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy</span>
</span></span><span style=display:flex><span>NAME     READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>tomcat   1/1     <span style=color:#ae81ff>1</span>            <span style=color:#ae81ff>1</span>           7m38s
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl delete deploy tomcat</span>
</span></span><span style=display:flex><span>deployment.apps <span style=color:#e6db74>&#34;tomcat&#34;</span> deleted
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy</span>
</span></span><span style=display:flex><span>No resources found in default namespace.
</span></span></code></pre></div><blockquote><p>kubectl api-resources 可以显示 k8s 的资源信息</p></blockquote><h4 id=多副本>多副本<a hidden class=anchor aria-hidden=true href=#多副本>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 命令行的方式</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl create deployment nginx --image=nginx --replicas=3</span>
</span></span><span style=display:flex><span>deployment.apps/nginx created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy</span>
</span></span><span style=display:flex><span>NAME    READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx   3/3     <span style=color:#ae81ff>3</span>            <span style=color:#ae81ff>3</span>           60s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6799fc88d8-bxptc   1/1     Running   <span style=color:#ae81ff>0</span>          62s
</span></span><span style=display:flex><span>nginx-6799fc88d8-fq296   1/1     Running   <span style=color:#ae81ff>0</span>          62s
</span></span><span style=display:flex><span>nginx-6799fc88d8-gjd8c   1/1     Running   <span style=color:#ae81ff>0</span>          62s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># yaml文件</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># vim nginx-deploy.yaml</span>
</span></span><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: nginx-deployment
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: <span style=color:#ae81ff>3</span>
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: nginx
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: nginx
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - name: nginx
</span></span><span style=display:flex><span>        image: nginx:1.14.2
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: <span style=color:#ae81ff>80</span>
</span></span></code></pre></div><p>到dashboard上看到刚刚创建的pod，也可以直接在dashboard上创建。</p><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/image-20230326130700708.png alt=image-20230326130700708></p><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/image-20230326131355770.png alt=image-20230326131355770></p><h4 id=扩缩容>扩缩容<a hidden class=anchor aria-hidden=true href=#扩缩容>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 扩容</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl scale deployment/nginx --replicas=5</span>
</span></span><span style=display:flex><span>deployment.apps/nginx scaled
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy</span>
</span></span><span style=display:flex><span>NAME     READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx    5/5     <span style=color:#ae81ff>5</span>            <span style=color:#ae81ff>5</span>           21m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz    1/1     Running   <span style=color:#ae81ff>0</span>          63s
</span></span><span style=display:flex><span>nginx-6799fc88d8-bxptc    1/1     Running   <span style=color:#ae81ff>0</span>          21m
</span></span><span style=display:flex><span>nginx-6799fc88d8-fq296    1/1     Running   <span style=color:#ae81ff>0</span>          21m
</span></span><span style=display:flex><span>nginx-6799fc88d8-gjd8c    1/1     Running   <span style=color:#ae81ff>0</span>          21m
</span></span><span style=display:flex><span>nginx-6799fc88d8-vsfzs    1/1     Running   <span style=color:#ae81ff>0</span>          63s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 缩容</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl scale deployment/nginx --replicas=2</span>
</span></span><span style=display:flex><span>deployment.apps/nginx scaled
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy</span>
</span></span><span style=display:flex><span>NAME     READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx    2/2     <span style=color:#ae81ff>2</span>            <span style=color:#ae81ff>2</span>           23m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                      READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz    1/1     Running   <span style=color:#ae81ff>0</span>          2m23s
</span></span><span style=display:flex><span>nginx-6799fc88d8-vsfzs    1/1     Running   <span style=color:#ae81ff>0</span>          2m23s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 也可以直接用 edit 的方式，修改 replicas的副本数</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl edit deployment nginx</span>
</span></span></code></pre></div><p>dashboard上操作</p><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/image-20230326132739579.png alt=image-20230326132739579></p><h4 id=自愈故障转移>自愈&故障转移<a hidden class=anchor aria-hidden=true href=#自愈故障转移>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -owide</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS    RESTARTS   AGE    IP                NODE        NOMINATED NODE   READINESS GATES
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz   1/1     Running   <span style=color:#ae81ff>0</span>          13m    192.168.36.71     k8s-node1   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>nginx-6799fc88d8-p9fw5   1/1     Running   <span style=color:#ae81ff>0</span>          7m1s   192.168.169.142   k8s-node2   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>nginx-6799fc88d8-vsfzs   1/1     Running   <span style=color:#ae81ff>0</span>          13m    192.168.36.70     k8s-node1   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 把node1上的容器停掉</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node1 ~<span style=color:#f92672>]</span><span style=color:#75715e># docker stop 11498d366d4d</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 观察 pod 的变化</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -w</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz   1/1     Running   <span style=color:#ae81ff>0</span>          14m
</span></span><span style=display:flex><span>nginx-6799fc88d8-p9fw5   1/1     Running   <span style=color:#ae81ff>0</span>          7m59s
</span></span><span style=display:flex><span>nginx-6799fc88d8-vsfzs   1/1     Running   <span style=color:#ae81ff>0</span>          14m
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz   0/1     Completed   <span style=color:#ae81ff>0</span>          14m
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz   1/1     Running     <span style=color:#ae81ff>1</span>          14m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># RESTARTS 变成1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz   1/1     Running   <span style=color:#ae81ff>1</span>          19m
</span></span><span style=display:flex><span>nginx-6799fc88d8-p9fw5   1/1     Running   <span style=color:#ae81ff>0</span>          12m
</span></span><span style=display:flex><span>nginx-6799fc88d8-vsfzs   1/1     Running   <span style=color:#ae81ff>0</span>          19m
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 假设node2节点宕机了</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -w</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz   1/1     Running   <span style=color:#ae81ff>1</span>          18m
</span></span><span style=display:flex><span>nginx-6799fc88d8-p9fw5   1/1     Running   <span style=color:#ae81ff>0</span>          12m
</span></span><span style=display:flex><span>nginx-6799fc88d8-vsfzs   1/1     Running   <span style=color:#ae81ff>0</span>          18m
</span></span><span style=display:flex><span>nginx-6799fc88d8-p9fw5   1/1     Running   <span style=color:#ae81ff>0</span>          13m
</span></span><span style=display:flex><span>nginx-6799fc88d8-p9fw5   1/1     Terminating   <span style=color:#ae81ff>0</span>          18m
</span></span><span style=display:flex><span>nginx-6799fc88d8-ghdsl   0/1     Pending       <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6799fc88d8-ghdsl   0/1     Pending       <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6799fc88d8-ghdsl   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6799fc88d8-ghdsl   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          1s
</span></span><span style=display:flex><span>nginx-6799fc88d8-ghdsl   1/1     Running             <span style=color:#ae81ff>0</span>          17s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 转移到了node1节点</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -owide</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS        RESTARTS   AGE   IP                NODE        NOMINATED NODE   READINESS GATES
</span></span><span style=display:flex><span>nginx-6799fc88d8-7r6tz   1/1     Running       <span style=color:#ae81ff>1</span>          25m   192.168.36.71     k8s-node1   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>nginx-6799fc88d8-ghdsl   1/1     Running       <span style=color:#ae81ff>0</span>          65s   192.168.36.72     k8s-node1   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>nginx-6799fc88d8-p9fw5   1/1     Terminating   <span style=color:#ae81ff>0</span>          19m   192.168.169.142   k8s-node2   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>nginx-6799fc88d8-vsfzs   1/1     Running       <span style=color:#ae81ff>0</span>          25m   192.168.36.70     k8s-node1   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><h4 id=滚动更新>滚动更新<a hidden class=anchor aria-hidden=true href=#滚动更新>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl set image deployment/nginx nginx=nginx:1.16.1 --record</span>
</span></span><span style=display:flex><span>deployment.apps/nginx image updated
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 滚动更新的过程</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -w</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS              RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6799fc88d8-2s6k7   1/1     Running             <span style=color:#ae81ff>0</span>          42s
</span></span><span style=display:flex><span>nginx-6799fc88d8-4tx9q   1/1     Running             <span style=color:#ae81ff>0</span>          42s
</span></span><span style=display:flex><span>nginx-6799fc88d8-g2jsk   1/1     Running             <span style=color:#ae81ff>0</span>          42s
</span></span><span style=display:flex><span>nginx-6889dfccd5-76299   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          3s
</span></span><span style=display:flex><span>nginx-6889dfccd5-76299   1/1     Running             <span style=color:#ae81ff>0</span>          17s
</span></span><span style=display:flex><span>nginx-6799fc88d8-4tx9q   1/1     Terminating         <span style=color:#ae81ff>0</span>          56s
</span></span><span style=display:flex><span>nginx-6889dfccd5-p4d54   0/1     Pending             <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6889dfccd5-p4d54   0/1     Pending             <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6889dfccd5-p4d54   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6799fc88d8-4tx9q   1/1     Terminating         <span style=color:#ae81ff>0</span>          56s
</span></span><span style=display:flex><span>nginx-6799fc88d8-4tx9q   0/1     Terminating         <span style=color:#ae81ff>0</span>          57s
</span></span><span style=display:flex><span>nginx-6889dfccd5-p4d54   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          1s
</span></span><span style=display:flex><span>nginx-6799fc88d8-4tx9q   0/1     Terminating         <span style=color:#ae81ff>0</span>          58s
</span></span><span style=display:flex><span>nginx-6799fc88d8-4tx9q   0/1     Terminating         <span style=color:#ae81ff>0</span>          67s
</span></span><span style=display:flex><span>nginx-6799fc88d8-4tx9q   0/1     Terminating         <span style=color:#ae81ff>0</span>          67s
</span></span><span style=display:flex><span>nginx-6889dfccd5-p4d54   1/1     Running             <span style=color:#ae81ff>0</span>          17s
</span></span><span style=display:flex><span>nginx-6799fc88d8-g2jsk   1/1     Terminating         <span style=color:#ae81ff>0</span>          73s
</span></span><span style=display:flex><span>nginx-6889dfccd5-55w2h   0/1     Pending             <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6889dfccd5-55w2h   0/1     Pending             <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6889dfccd5-55w2h   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          0s
</span></span><span style=display:flex><span>nginx-6799fc88d8-g2jsk   1/1     Terminating         <span style=color:#ae81ff>0</span>          74s
</span></span><span style=display:flex><span>nginx-6799fc88d8-g2jsk   0/1     Terminating         <span style=color:#ae81ff>0</span>          74s
</span></span><span style=display:flex><span>nginx-6889dfccd5-55w2h   0/1     ContainerCreating   <span style=color:#ae81ff>0</span>          1s
</span></span><span style=display:flex><span>nginx-6799fc88d8-g2jsk   0/1     Terminating         <span style=color:#ae81ff>0</span>          77s
</span></span><span style=display:flex><span>nginx-6799fc88d8-g2jsk   0/1     Terminating         <span style=color:#ae81ff>0</span>          77s
</span></span><span style=display:flex><span>nginx-6889dfccd5-55w2h   1/1     Running             <span style=color:#ae81ff>0</span>          17s
</span></span><span style=display:flex><span>nginx-6799fc88d8-2s6k7   1/1     Terminating         <span style=color:#ae81ff>0</span>          90s
</span></span><span style=display:flex><span>nginx-6799fc88d8-2s6k7   1/1     Terminating         <span style=color:#ae81ff>0</span>          91s
</span></span><span style=display:flex><span>nginx-6799fc88d8-2s6k7   0/1     Terminating         <span style=color:#ae81ff>0</span>          91s
</span></span><span style=display:flex><span>nginx-6799fc88d8-2s6k7   0/1     Terminating         <span style=color:#ae81ff>0</span>          92s
</span></span><span style=display:flex><span>nginx-6799fc88d8-2s6k7   0/1     Terminating         <span style=color:#ae81ff>0</span>          92s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>nginx-6889dfccd5-55w2h   1/1     Running   <span style=color:#ae81ff>0</span>          21s
</span></span><span style=display:flex><span>nginx-6889dfccd5-76299   1/1     Running   <span style=color:#ae81ff>0</span>          55s
</span></span><span style=display:flex><span>nginx-6889dfccd5-p4d54   1/1     Running   <span style=color:#ae81ff>0</span>          38s
</span></span></code></pre></div><h4 id=版本回退>版本回退<a hidden class=anchor aria-hidden=true href=#版本回退>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 查看历史记录</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl rollout history deployment/nginx</span>
</span></span><span style=display:flex><span>deployment.apps/nginx 
</span></span><span style=display:flex><span>REVISION  CHANGE-CAUSE
</span></span><span style=display:flex><span><span style=color:#ae81ff>1</span>         &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#ae81ff>2</span>         kubectl set image deployment/nginx nginx<span style=color:#f92672>=</span>nginx:1.16.1 --record<span style=color:#f92672>=</span>true
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看历史详情</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl rollout history deployment/nginx --revision=2</span>
</span></span><span style=display:flex><span>deployment.apps/nginx with revision <span style=color:#75715e>#2</span>
</span></span><span style=display:flex><span>Pod Template:
</span></span><span style=display:flex><span>  Labels:	app<span style=color:#f92672>=</span>nginx
</span></span><span style=display:flex><span>	pod-template-hash<span style=color:#f92672>=</span>6889dfccd5
</span></span><span style=display:flex><span>  Annotations:	kubernetes.io/change-cause: kubectl set image deployment/nginx nginx<span style=color:#f92672>=</span>nginx:1.16.1 --record<span style=color:#f92672>=</span>true
</span></span><span style=display:flex><span>  Containers:
</span></span><span style=display:flex><span>   nginx:
</span></span><span style=display:flex><span>    Image:	nginx:1.16.1
</span></span><span style=display:flex><span>    Port:	&lt;none&gt;
</span></span><span style=display:flex><span>    Host Port:	&lt;none&gt;
</span></span><span style=display:flex><span>    Environment:	&lt;none&gt;
</span></span><span style=display:flex><span>    Mounts:	&lt;none&gt;
</span></span><span style=display:flex><span>  Volumes:	&lt;none&gt;
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#75715e># 回退到上个版本，也可以指定 --reversion=2</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl rollout undo deployment/nginx</span>
</span></span><span style=display:flex><span>deployment.apps/nginx rolled back
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看已经回退到了上个版本</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy/nginx -oyaml |grep image</span>
</span></span><span style=display:flex><span>                f:imagePullPolicy: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>                f:image: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>      - image: nginx
</span></span><span style=display:flex><span>        imagePullPolicy: Always
</span></span></code></pre></div><h3 id=daemonset>DaemonSet<a hidden class=anchor aria-hidden=true href=#daemonset>#</a></h3><p>DaemonSet 确保全部（或者某些）节点上运行一个 Pod 的副本。 当有节点加入集群时， 也会为他们新增一个 Pod 。 当有节点从集群移除时，这些 Pod 也会被回收。删除 DaemonSet 将会删除它创建的所有 Pod。在形式上和 Deployment 类似，都是管理控制 Pod，但管理调度策略却不同。</p><p>DaemonSet 的一些典型用法：</p><ul><li>网络应用（如 kube-proxy），必须每个节点都运行一个 Pod，否则节点就无法加入 Kubernetes 网络。</li><li>监控应用（如 Prometheus），必须每个节点都有一个 Pod，用来监控节点的状态，实时上报信息。</li><li>日志应用（如 Fluentd），必须在每个节点上运行一个 Pod，才能够搜集容器运行时产生的日志数据。</li></ul><p>kubectl 不提供自动创建 DaemonSet YAML 样板的功能，也就是说，我们不能用命令 kubectl create 直接创建出一个 DaemonSet 对象。</p><h4 id=使用daemonset>使用DaemonSet<a hidden class=anchor aria-hidden=true href=#使用daemonset>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl create ds</span>
</span></span><span style=display:flex><span>Error: must specify one of -f and -k
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>error: unknown command <span style=color:#e6db74>&#34;ds&#34;</span>
</span></span><span style=display:flex><span>See <span style=color:#e6db74>&#39;kubectl create -h&#39;</span> <span style=color:#66d9ef>for</span> help and examples
</span></span></code></pre></div><p>DaemonSet 的YAML文件和 Deployment 的YAML文件是几乎一模一样的，差别在于DaemonSet 的YAML文件中没有 replicas 字段，所以我们可以先用 kubectl create deploy 创建一个 Deployment 对象，再把 kind 改成 DaemonSet，删除 spec.replicas 就行了。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl create deploy redis-ds --image=redis:5-alpine --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: Deployment
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: null
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: redis-ds
</span></span><span style=display:flex><span>  name: redis-ds
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  replicas: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: redis-ds
</span></span><span style=display:flex><span>  strategy: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      creationTimestamp: null
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: redis-ds
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: redis:5-alpine
</span></span><span style=display:flex><span>        name: redis
</span></span><span style=display:flex><span>        resources: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>status: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 修改以后变成以下 ds.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># cat ds.yaml </span>
</span></span><span style=display:flex><span>apiVersion: apps/v1
</span></span><span style=display:flex><span>kind: DaemonSet
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: redis-ds
</span></span><span style=display:flex><span>  name: redis-ds
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    matchLabels:
</span></span><span style=display:flex><span>      app: redis-ds
</span></span><span style=display:flex><span>  template:
</span></span><span style=display:flex><span>    metadata:
</span></span><span style=display:flex><span>      labels:
</span></span><span style=display:flex><span>        app: redis-ds
</span></span><span style=display:flex><span>    spec:
</span></span><span style=display:flex><span>      containers:
</span></span><span style=display:flex><span>      - image: redis:5-alpine
</span></span><span style=display:flex><span>        name: redis
</span></span><span style=display:flex><span>        ports:
</span></span><span style=display:flex><span>        - containerPort: <span style=color:#ae81ff>6379</span>
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f ds.yaml </span>
</span></span><span style=display:flex><span>daemonset.apps/redis-ds created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 看到生成了两个Pod，运行在node节点上，Master默认是不跑应用的</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get ds</span>
</span></span><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
</span></span><span style=display:flex><span>redis-ds   <span style=color:#ae81ff>2</span>         <span style=color:#ae81ff>2</span>         <span style=color:#ae81ff>1</span>       <span style=color:#ae81ff>2</span>            <span style=color:#ae81ff>1</span>           &lt;none&gt;          5s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -owide</span>
</span></span><span style=display:flex><span>redis-ds-9vwjg           1/1     Running   <span style=color:#ae81ff>0</span>          2m23s   192.168.169.132   k8s-node2   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>redis-ds-z47hg           1/1     Running   <span style=color:#ae81ff>0</span>          2m23s   192.168.36.68     k8s-node1   &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><p>DaemonSet 应该在每个节点上都运行一个 Pod 实例才对，但 Master 节点却被排除在外了，这就不符合我们当初的设想了。为了应对 Pod 在某些节点的“调度”和“驱逐”问题，它定义了两个新的概念：污点（taint）和容忍度（toleration）。</p><h4 id=污点taint和容忍度toleration>污点（taint）和容忍度（toleration）<a hidden class=anchor aria-hidden=true href=#污点taint和容忍度toleration>#</a></h4><p>“污点”是 Kubernetes 节点的一个属性，它的作用也是给节点“贴标签”，但为了不和已有的 labels 字段混淆，就改成了 taint。</p><p>和“污点”相对的，就是 Pod 的“容忍度”，顾名思义，就是 Pod 能否“容忍”污点。</p><p>Kubernetes 在创建集群的时候会自动给节点 Node 加上一些“污点”，方便 Pod 的调度和部署。你可以用 kubectl describe node 来查看 Master 和 Node 的状态：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl describe node</span>
</span></span><span style=display:flex><span>Name:               k8s-master
</span></span><span style=display:flex><span>Roles:              control-plane,master
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Taints:             node-role.kubernetes.io/master:NoSchedule
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:               k8s-node1
</span></span><span style=display:flex><span>Roles:              &lt;none&gt;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Taints:             &lt;none&gt;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Name:               k8s-node2
</span></span><span style=display:flex><span>Roles:              &lt;none&gt;
</span></span><span style=display:flex><span>...
</span></span><span style=display:flex><span>Taints:             &lt;none&gt;
</span></span><span style=display:flex><span>...
</span></span></code></pre></div><p>可以看到，Master 节点默认有一个 taint，名字是 node-role.kubernetes.io/master，它的效果是 NoSchedule，也就是说这个污点会拒绝 Pod 调度到本节点上运行，而 Node 节点的 taint 字段则是空的。</p><p>这正是 Master 和 Worker 在 Pod 调度策略上的区别所在，通常来说 Pod 都不能容忍任何“污点”，所以加上了 taint 属性的 Master 节点也就会无缘 Pod 了。</p><p>让 DaemonSet 在 Master 节点（或者任意其他节点）上运行了，方法有两种：</p><ul><li><p>去掉 Master 节点上的 taint</p></li><li><p>为 Pod 添加字段 tolerations，让它能够“容忍”某些“污点”。</p></li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 第一种方法</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule-</span>
</span></span><span style=display:flex><span>node/k8s-master untainted
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get ds</span>
</span></span><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
</span></span><span style=display:flex><span>redis-ds   <span style=color:#ae81ff>3</span>         <span style=color:#ae81ff>3</span>         <span style=color:#ae81ff>3</span>       <span style=color:#ae81ff>3</span>            <span style=color:#ae81ff>3</span>           &lt;none&gt;          21m
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -owide</span>
</span></span><span style=display:flex><span>redis-ds-672rq           1/1     Running   <span style=color:#ae81ff>0</span>          30s     192.168.235.196   k8s-master   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>redis-ds-9vwjg           1/1     Running   <span style=color:#ae81ff>0</span>          21m     192.168.169.132   k8s-node2    &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>redis-ds-z47hg           1/1     Running   <span style=color:#ae81ff>0</span>          21m     192.168.36.68     k8s-node1    &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 第二种方法</span>
</span></span><span style=display:flex><span><span style=color:#75715e># 先把Master的“污点”加上</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># ds.yaml 里的 ds.spec.template.spec.tolerations 加入以下字段</span>
</span></span><span style=display:flex><span>tolerations:
</span></span><span style=display:flex><span>- key: node-role.kubernetes.io/master
</span></span><span style=display:flex><span>  effect: NoSchedule
</span></span><span style=display:flex><span>  operator: Exists
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#75715e># 重新部署加上了“容忍度”的 DaemonSet</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f ds.yaml</span>
</span></span><span style=display:flex><span>daemonset.apps/redis-ds created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 看到仍然还是有3个Pod分别运行在每个节点上</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get ds</span>
</span></span><span style=display:flex><span>NAME       DESIRED   CURRENT   READY   UP-TO-DATE   AVAILABLE   NODE SELECTOR   AGE
</span></span><span style=display:flex><span>redis-ds   <span style=color:#ae81ff>3</span>         <span style=color:#ae81ff>3</span>         <span style=color:#ae81ff>3</span>       <span style=color:#ae81ff>3</span>            <span style=color:#ae81ff>3</span>           &lt;none&gt;          13s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -owide</span>
</span></span><span style=display:flex><span>redis-ds-7l5j5           1/1     Running   <span style=color:#ae81ff>0</span>          20s     192.168.235.197   k8s-master   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>redis-ds-b2h4b           1/1     Running   <span style=color:#ae81ff>0</span>          20s     192.168.36.69     k8s-node1    &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>redis-ds-gljb4           1/1     Running   <span style=color:#ae81ff>0</span>          20s     192.168.169.133   k8s-node2    &lt;none&gt;           &lt;none&gt;
</span></span></code></pre></div><h3 id=statefulset>StatefulSet<a hidden class=anchor aria-hidden=true href=#statefulset>#</a></h3><h2 id=服务发现与负载均衡>服务发现与负载均衡<a hidden class=anchor aria-hidden=true href=#服务发现与负载均衡>#</a></h2><h3 id=service>Service<a hidden class=anchor aria-hidden=true href=#service>#</a></h3><p>Kubernetes 中 Pod 的生命周期是比较“短暂”的，虽然 Deployment 和 DaemonSet 可以维持 Pod 总体数量的稳定，但在运行过程中，难免会有 Pod 销毁又重建，这就会导致 Pod 集合处于动态的变化之中。</p><p>这导致了一个问题： 如果一组 Pod（称为“后端”）为集群内的其他 Pod（称为“前端”）提供功能， 那么前端如何找出并跟踪要连接的 IP 地址，以便前端可以使用提供工作负载的后端部分？</p><p>Kubernetes 中 Service 是将运行在一个或一组 Pod 上的网络应用程序公开为网络服务的方法。</p><p>Service 本身是没有服务能力的，它只是一些 iptables 规则，真正配置、应用这些规则的实际上是节点里的 kube-proxy 组件。</p><p>Service 的工作原理示意图：</p><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/Service.png alt=Service.png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 修改每个nginx的/usr/share/nginx/html/index.html文件</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod -owide</span>
</span></span><span style=display:flex><span>NAME                     READY   STATUS    RESTARTS   AGE   IP                NODE        NOMINATED NODE   READINESS GATES
</span></span><span style=display:flex><span>nginx-6799fc88d8-5sdkc   1/1     Running   <span style=color:#ae81ff>0</span>          19m   192.168.169.130   k8s-node2   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>nginx-6799fc88d8-s9wgx   1/1     Running   <span style=color:#ae81ff>0</span>          19m   192.168.36.65     k8s-node1   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span>nginx-6799fc88d8-w6q4p   1/1     Running   <span style=color:#ae81ff>0</span>          19m   192.168.169.129   k8s-node2   &lt;none&gt;           &lt;none&gt;
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 192.168.169.130</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1111</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 192.168.36.65</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2222</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 192.168.169.129</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3333</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --dry-run=client -o yaml 以yaml的形式输出</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl expose deploy nginx --port=8000 --target-port=80 --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: null
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>  - port: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>    protocol: TCP
</span></span><span style=display:flex><span>    targetPort: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  loadBalancer: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 暴露 deploy</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl expose deploy nginx --port=8000 --target-port=80</span>
</span></span><span style=display:flex><span>service/nginx exposed
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get svc</span>
</span></span><span style=display:flex><span>NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#f92672>(</span>S<span style=color:#f92672>)</span>    AGE
</span></span><span style=display:flex><span>kubernetes   ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP    34m
</span></span><span style=display:flex><span>nginx        ClusterIP   10.96.103.120   &lt;none&gt;        8000/TCP   12s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 集群内使用 CLUSTER-IP 访问</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 10.96.103.120:8000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3333</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 10.96.103.120:8000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3333</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 10.96.103.120:8000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>3333</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 10.96.103.120:8000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>2222</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 10.96.103.120:8000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1111</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># curl 10.96.103.120:8000</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>1111</span>
</span></span></code></pre></div><h4 id=clusterip>ClusterIP<a hidden class=anchor aria-hidden=true href=#clusterip>#</a></h4><p>&ndash;type=ClusterIP 集群内部访问</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 默认是 ClusterIP ，也可以加上 --type=ClusterIP </span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl expose deploy nginx --port=8000 --target-port=80 --type=ClusterIP --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: null
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>  - port: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>    protocol: TCP
</span></span><span style=display:flex><span>    targetPort: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>  type: ClusterIP
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  loadBalancer: <span style=color:#f92672>{}</span>
</span></span></code></pre></div><h4 id=nodeport>NodePort<a hidden class=anchor aria-hidden=true href=#nodeport>#</a></h4><p>&ndash;type=NodePort 集群外部也可以访问</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl expose deploy nginx --port=8000 --target-port=80 --type=NodePort --target-port=80 --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: Service
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  creationTimestamp: null
</span></span><span style=display:flex><span>  labels:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>  name: nginx
</span></span><span style=display:flex><span>spec:
</span></span><span style=display:flex><span>  ports:
</span></span><span style=display:flex><span>  - port: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>    protocol: TCP
</span></span><span style=display:flex><span>    targetPort: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>  selector:
</span></span><span style=display:flex><span>    app: nginx
</span></span><span style=display:flex><span>  type: NodePort
</span></span><span style=display:flex><span>status:
</span></span><span style=display:flex><span>  loadBalancer: <span style=color:#f92672>{}</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># PORT 那里多了个32018的端口</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl expose deploy nginx --port=8000 --target-port=80 --type=NodePort --target-port=80</span>
</span></span><span style=display:flex><span>service/nginx exposed
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get svc</span>
</span></span><span style=display:flex><span>NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT<span style=color:#f92672>(</span>S<span style=color:#f92672>)</span>          AGE
</span></span><span style=display:flex><span>kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443/TCP          58m
</span></span><span style=display:flex><span>nginx        NodePort    10.96.64.157   &lt;none&gt;        8000:32018/TCP   8s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 浏览器访问任一节点的公网IP:32018</span>
</span></span></code></pre></div><blockquote><p>NodePort 范围在 30000-32767 之间</p></blockquote><h3 id=ingress>Ingress<a hidden class=anchor aria-hidden=true href=#ingress>#</a></h3><p>Ingress 公开从集群外部到集群内服务的 HTTP 和 HTTPS 路由。 流量路由由 Ingress 资源上定义的规则控制。</p><p>Service 本质上就是一个由 kube-proxy 控制的四层负载均衡，在 TCP/IP 协议栈上转发流量。但在四层上的负载均衡功能还是太有限了，只能够依据 IP 地址和端口号做一些简单的判断和组合，而我们现在的绝大多数应用都是跑在七层的 HTTP/HTTPS 协议上的，有更多的高级路由条件，比如主机名、URI、请求头、证书等等，而这些在 TCP/IP 网络栈里是根本看不见的。</p><p>于是就引入了 Ingress，它是在七层上做负载均衡，除了七层负载均衡，也作为流量的总入口，统管集群的进出口数据。</p><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/Ingress.png alt=Ingress.png></p><p>Ingress 也只是一些 HTTP 路由规则的集合，相当于一份静态的描述文件，真正要把这些规则在集群里实施运行，还需要有另外一个东西，这就是 Ingress Controller，它的作用就相当于 Service 的 kube-proxy，能够读取、应用 Ingress 规则，处理、调度流量。</p><h4 id=安装>安装<a hidden class=anchor aria-hidden=true href=#安装>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v0.47.0/deploy/static/provider/baremetal/deploy.yaml</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f deploy.yaml</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get svc -A</span>
</span></span><span style=display:flex><span>NAMESPACE              NAME                                 TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#f92672>(</span>S<span style=color:#f92672>)</span>                      AGE
</span></span><span style=display:flex><span>default                kubernetes                           ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP                      18h
</span></span><span style=display:flex><span>default                nginx                                NodePort    10.96.64.157    &lt;none&gt;        8000:32018/TCP               18h
</span></span><span style=display:flex><span>ingress-nginx          ingress-nginx-controller             NodePort    10.96.177.2     &lt;none&gt;        80:31043/TCP,443:31379/TCP   13m
</span></span><span style=display:flex><span>ingress-nginx          ingress-nginx-controller-admission   ClusterIP   10.96.194.45    &lt;none&gt;        443/TCP                      13m
</span></span><span style=display:flex><span>kube-system            kube-dns                             ClusterIP   10.96.0.10      &lt;none&gt;        53/UDP,53/TCP,9153/TCP       18h
</span></span><span style=display:flex><span>kubernetes-dashboard   dashboard-metrics-scraper            ClusterIP   10.96.188.220   &lt;none&gt;        8000/TCP                     18h
</span></span><span style=display:flex><span>kubernetes-dashboard   kubernetes-dashboard                 NodePort    10.96.164.146   &lt;none&gt;        443:32747/TCP                18h
</span></span></code></pre></div><p>浏览器访问任一节点的 http://公网IP:31043，https://公网IP:31379</p><h4 id=测试域名访问>测试域名访问<a hidden class=anchor aria-hidden=true href=#测试域名访问>#</a></h4><p>tomcat.aliyun.com:31043请求转给tomcat-demo处理</p><p>nginx.aliyun.com:31043请求转给nginx-demo处理</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># 准备一个test.yaml的文件</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# cat test.yaml </span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tomcat</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>image</span>: <span style=color:#ae81ff>tomcat</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>80</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Service</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>ports</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>port</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>protocol</span>: <span style=color:#ae81ff>TCP</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>targetPort</span>: <span style=color:#ae81ff>9000</span>
</span></span><span style=display:flex><span>    
</span></span><span style=display:flex><span><span style=color:#75715e># ingress.yaml文件</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# cat ingress.yaml </span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress  </span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>ingress-host-bar</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ingressClassName</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>host</span>: <span style=color:#e6db74>&#34;tomcat.aliyun.com&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>number</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>host</span>: <span style=color:#e6db74>&#34;nginx.aliyun.com&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>number</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>              
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 本地hosts文件新增</span>
</span></span><span style=display:flex><span>公网IP	tomcat.aliyun.com
</span></span><span style=display:flex><span>公网IP	nginx.aliyun.com
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get svc</span>
</span></span><span style=display:flex><span>NAME          TYPE        CLUSTER-IP      EXTERNAL-IP   PORT<span style=color:#f92672>(</span>S<span style=color:#f92672>)</span>          AGE
</span></span><span style=display:flex><span>kubernetes    ClusterIP   10.96.0.1       &lt;none&gt;        443/TCP          20h
</span></span><span style=display:flex><span>nginx         NodePort    10.96.64.157    &lt;none&gt;        8000:32018/TCP   19h
</span></span><span style=display:flex><span>nginx-demo    ClusterIP   10.96.161.217   &lt;none&gt;        8000/TCP         12s
</span></span><span style=display:flex><span>tomcat-demo   ClusterIP   10.96.149.244   &lt;none&gt;        8000/TCP         12s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy</span>
</span></span><span style=display:flex><span>NAME          READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>nginx-demo    2/2     <span style=color:#ae81ff>2</span>            <span style=color:#ae81ff>2</span>           20s
</span></span><span style=display:flex><span>tomcat-demo   1/2     <span style=color:#ae81ff>2</span>            <span style=color:#ae81ff>1</span>           20s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get ing</span>
</span></span><span style=display:flex><span>NAME               CLASS   HOSTS                                ADDRESS   PORTS   AGE
</span></span><span style=display:flex><span>ingress-host-bar   nginx   tomcat.aliyun.com,nginx.aliyun.com             <span style=color:#ae81ff>80</span>      18s
</span></span></code></pre></div><p>浏览器访问域名:31043</p><h4 id=路径重写>路径重写<a hidden class=anchor aria-hidden=true href=#路径重写>#</a></h4><p><a href=https://kubernetes.github.io/ingress-nginx/examples/rewrite/>官网地址</a>有详细介绍</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# cat ingress.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress  </span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>nginx.ingress.kubernetes.io/rewrite-target</span>: <span style=color:#ae81ff>/$2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>ingress-host-bar</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ingressClassName</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>host</span>: <span style=color:#e6db74>&#34;tomcat.aliyun.com&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>tomcat-demo</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>number</span>: <span style=color:#ae81ff>8000</span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>host</span>: <span style=color:#e6db74>&#34;nginx.aliyun.com&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Prefix</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/nginx(/|$)(.*)&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>number</span>: <span style=color:#ae81ff>8000</span>
</span></span></code></pre></div><h4 id=流量限制>流量限制<a hidden class=anchor aria-hidden=true href=#流量限制>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>networking.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Ingress</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>ingress-limit-rate</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>annotations</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>nginx.ingress.kubernetes.io/limit-rps</span>: <span style=color:#e6db74>&#34;1&#34;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>ingressClassName</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>host</span>: <span style=color:#e6db74>&#34;nginx.aliyun.com&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>http</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>paths</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>pathType</span>: <span style=color:#ae81ff>Exact</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#34;/&#34;</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>backend</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>service</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-demo</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>port</span>:
</span></span><span style=display:flex><span>              <span style=color:#f92672>number</span>: <span style=color:#ae81ff>8000</span>
</span></span></code></pre></div><h2 id=存储>存储<a hidden class=anchor aria-hidden=true href=#存储>#</a></h2><h3 id=persistentvolume>PersistentVolume<a hidden class=anchor aria-hidden=true href=#persistentvolume>#</a></h3><p>持久卷（PersistentVolume，PV）将应用需要持久化的数据保存到指定位置。PV 实际上就是一些存储设备、文件系统，比如 Ceph、GlusterFS、NFS，甚至是本地磁盘。</p><p>简单 YAML 实例</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# vim pv.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolume</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>host-10m-pv</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>host-test</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>  - <span style=color:#ae81ff>ReadWriteOnce</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>capacity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>10Mi</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>hostPath</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/tmp/host-10m-pv/</span>
</span></span><span style=display:flex><span>    
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 创建PV</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f pv.yaml </span>
</span></span><span style=display:flex><span>persistentvolume/host-10m-pv created
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pv</span>
</span></span><span style=display:flex><span>NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
</span></span><span style=display:flex><span>host-10m-pv   10Mi       RWO            Retain           Available           host-test               6s
</span></span></code></pre></div><h3 id=persistentvolumeclaim>PersistentVolumeClaim<a hidden class=anchor aria-hidden=true href=#persistentvolumeclaim>#</a></h3><p>持久卷申领（PersistentVolumeClaim，PVC）申明需要使用的持久卷规格。（例如，可以要求 PV 卷能够以 ReadWriteOnce、ReadOnlyMany 或 ReadWriteMany 模式之一来挂载。</p><ul><li>ReadWriteOnce：存储卷可读可写，但只能被一个节点上的 Pod 挂载。</li><li>ReadOnlyMany：存储卷只读不可写，可以被任意节点上的 Pod 多次挂载。</li><li>ReadWriteMany：存储卷可读可写，也可以被任意节点上的 Pod 多次挂载。</li></ul><p>简单 YAML 实例</p><p>PVC 的内容与 PV 很像，但它不表示实际的存储，而是一个“申请”或者“声明”，spec 里的字段描述的是对存储的“期望状态”。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# vim pvc.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolumeClaim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>host-5m-pvc</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>host-test</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteOnce</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>5Mi</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 创建PVC</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f pvc.yaml </span>
</span></span><span style=display:flex><span>persistentvolumeclaim/host-5m-pvc created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pvc</span>
</span></span><span style=display:flex><span>NAME          STATUS   VOLUME        CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>host-5m-pvc   Bound    host-10m-pv   10Mi       RWO            host-test      3s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pv</span>
</span></span><span style=display:flex><span>NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                 STORAGECLASS   REASON   AGE
</span></span><span style=display:flex><span>host-10m-pv   10Mi       RWO            Retain           Bound    default/host-5m-pvc   host-test               2m53s
</span></span></code></pre></div><p>一旦 PVC 对象创建成功，Kubernetes 就会立即通过 StorageClass、resources 等条件在集群里查找符合要求的 PV，如果找到合适的存储对象就会把它俩“绑定”在一起。看到这两个对象的状态都是 Bound，也就是说存储申请成功。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 把 PVC 的申请容量改成 100MB</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pv</span>
</span></span><span style=display:flex><span>NAME          CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS   REASON   AGE
</span></span><span style=display:flex><span>host-10m-pv   10Mi       RWO            Retain           Available           host-test               9s
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pvc</span>
</span></span><span style=display:flex><span>NAME          STATUS    VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>host-5m-pvc   Pending                                      host-test      7s
</span></span></code></pre></div><p>看到 PVC 会一直处于 Pending 状态，这意味着 Kubernetes 在系统里没有找到符合要求的存储，无法分配资源，只能等有满足要求的 PV 才能完成绑定。</p><h3 id=nfs>NFS<a hidden class=anchor aria-hidden=true href=#nfs>#</a></h3><p>前面使用的是 HostPath，存储卷只能在本机使用。要想让存储卷真正能被 Pod 任意挂载，需要改成网络存储。</p><p><strong>安装NFS</strong></p><p>在所有节点安装</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>yum install -y nfs-utils
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 创建网络共享目录</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># mkdir -p /nfs/data</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 配置 NFS 访问共享目录</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># echo &#34;/nfs/data/ *(insecure,rw,sync,no_root_squash)&#34; &gt; /etc/exports</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 让配置生效，验证效果</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># exportfs -r</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># exportfs -v</span>
</span></span><span style=display:flex><span>/nfs/data     	&lt;world&gt;<span style=color:#f92672>(</span>sync,wdelay,hide,no_subtree_check,sec<span style=color:#f92672>=</span>sys,rw,no_root_squash,no_all_squash<span style=color:#f92672>)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 启动 NFS 服务器</span>
</span></span><span style=display:flex><span>systemctl enable rpcbind --now
</span></span><span style=display:flex><span>systemctl enable nfs-server --now
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># node 节点上查看</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node1 ~<span style=color:#f92672>]</span><span style=color:#75715e># showmount -e 172.21.252.80</span>
</span></span><span style=display:flex><span>Export list <span style=color:#66d9ef>for</span> 172.21.252.80:
</span></span><span style=display:flex><span>/nfs/data *
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 创建网络存储挂载点</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node1 ~<span style=color:#f92672>]</span><span style=color:#75715e># mkdir -p /nfs/data</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node2 ~<span style=color:#f92672>]</span><span style=color:#75715e># mkdir -p /nfs/data</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 把 NFS 服务器的共享目录挂载到/nfs/data</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node1 ~<span style=color:#f92672>]</span><span style=color:#75715e># mount -t nfs 172.21.252.80:/nfs/data /nfs/data</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node2 ~<span style=color:#f92672>]</span><span style=color:#75715e># mount -t nfs 172.21.252.80:/nfs/data /nfs/data</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 测试</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-node1 ~<span style=color:#f92672>]</span><span style=color:#75715e># echo &#34;nfs server&#34; &gt; /nfs/data/nfs.txt</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master data<span style=color:#f92672>]</span><span style=color:#75715e># ls</span>
</span></span><span style=display:flex><span>nfs.txt
</span></span></code></pre></div><h3 id=使用nfs静态存储卷>使用NFS静态存储卷<a hidden class=anchor aria-hidden=true href=#使用nfs静态存储卷>#</a></h3><p><strong>创建PV池</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># mkdir -p /nfs/data/{01,02,03}</span>
</span></span></code></pre></div><p><strong>使用NFS创建PV</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master data]# vim mult-pv.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolume</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pv01-10m</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>capacity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>10M</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteMany</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>nfs</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>nfs</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/nfs/data/01</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>server</span>: <span style=color:#ae81ff>172.21.252.80</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolume</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pv02-1gi</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>capacity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>1Gi</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteMany</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>nfs</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>nfs</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/nfs/data/02</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>server</span>: <span style=color:#ae81ff>172.21.252.80</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolume</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>pv03-3gi</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>capacity</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>3Gi</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteMany</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>nfs</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>nfs</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/nfs/data/03</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>server</span>: <span style=color:#ae81ff>172.21.252.80</span>
</span></span></code></pre></div><p><strong>使用NFS创建PVC并绑定</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master data]# cat mult-pvc.yaml </span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolumeClaim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-pvc</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteMany</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>200Mi</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>nfs</span>
</span></span></code></pre></div><p><strong>创建Pod绑定PVC</strong></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-deploy-pvc</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-deploy-pvc</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>2</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-deploy-pvc</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nginx-deploy-pvc</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>image</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>html</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/usr/share/nginx/html</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>html</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>persistentVolumeClaim</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>claimName</span>: <span style=color:#ae81ff>nginx-pvc</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f mult-pv.yaml</span>
</span></span><span style=display:flex><span>persistentvolume/pv01-10m created
</span></span><span style=display:flex><span>persistentvolume/pv02-1gi created
</span></span><span style=display:flex><span>persistentvolume/pv03-3gi created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f mult-pvc.yaml</span>
</span></span><span style=display:flex><span>persistentvolumeclaim/nginx-pvc created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pv,pvc</span>
</span></span><span style=display:flex><span>NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM               STORAGECLASS   REASON   AGE
</span></span><span style=display:flex><span>persistentvolume/host-10m-pv   10Mi       RWO            Retain           Available                       host-test               81m
</span></span><span style=display:flex><span>persistentvolume/pv01-10m      10M        RWX            Retain           Available                       nfs                     19s
</span></span><span style=display:flex><span>persistentvolume/pv02-1gi      1Gi        RWX            Retain           Bound       default/nginx-pvc   nfs                     19s
</span></span><span style=display:flex><span>persistentvolume/pv03-3gi      3Gi        RWX            Retain           Available                       nfs                     19s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NAME                                STATUS    VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>persistentvolumeclaim/host-5m-pvc   Pending                                        host-test      81m
</span></span><span style=display:flex><span>persistentvolumeclaim/nginx-pvc     Bound     pv02-1gi   1Gi        RWX            nfs            15s
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f nginx-pvc.yaml</span>
</span></span><span style=display:flex><span>deployment.apps/nginx-deploy-pvc created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 可以看到刚创建的pod和 pv02-1gi 绑定了</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pv,pvc</span>
</span></span><span style=display:flex><span>NAME                           CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM               STORAGECLASS   REASON   AGE
</span></span><span style=display:flex><span>persistentvolume/host-10m-pv   10Mi       RWO            Retain           Available                       host-test               82m
</span></span><span style=display:flex><span>persistentvolume/pv01-10m      10M        RWX            Retain           Available                       nfs                     41s
</span></span><span style=display:flex><span>persistentvolume/pv02-1gi      1Gi        RWX            Retain           Bound       default/nginx-pvc   nfs                     41s
</span></span><span style=display:flex><span>persistentvolume/pv03-3gi      3Gi        RWX            Retain           Available                       nfs                     41s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>NAME                                STATUS    VOLUME     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>persistentvolumeclaim/host-5m-pvc   Pending                                        host-test      81m
</span></span><span style=display:flex><span>persistentvolumeclaim/nginx-pvc     Bound     pv02-1gi   1Gi        RWX            nfs            37s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 进入pod新增一个文件，再回到主机的/nfs/data/02目录下查看</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master 02<span style=color:#f92672>]</span><span style=color:#75715e># kubectl exec -it nginx-deploy-pvc-79fc8558c7-wzzfl -- /bin/bash</span>
</span></span><span style=display:flex><span>root@nginx-deploy-pvc-79fc8558c7-wzzfl:/# cd /usr/share/nginx/html/
</span></span><span style=display:flex><span>root@nginx-deploy-pvc-79fc8558c7-wzzfl:/usr/share/nginx/html# ls
</span></span><span style=display:flex><span>root@nginx-deploy-pvc-79fc8558c7-wzzfl:/usr/share/nginx/html# echo <span style=color:#ae81ff>111</span> &gt; index.html
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># cat /nfs/data/02/index.html </span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>111</span>
</span></span></code></pre></div><h3 id=使用nfs动态存储卷>使用NFS动态存储卷<a hidden class=anchor aria-hidden=true href=#使用nfs动态存储卷>#</a></h3><p>在一个大集群里，每天可能会有几百几千个应用需要 PV 存储，PV 人工管理不太现实。而且 PV 的大小也很难精确控制，容易出现空间不足或者空间浪费的情况。</p><p>动态存储卷它可以用 StorageClass 绑定一个 Provisioner 对象，而这个 Provisioner 就是一个能够自动管理存储、创建 PV 的应用，代替了原来系统管理员的手工劳动。</p><p>目前，Kubernetes 里每类存储设备都有相应的 Provisioner 对象，对于 NFS 来说，它的 Provisioner 就是“NFS subdir external provisioner”，<a href=https://github.com/kubernetes-sigs/nfs-subdir-external-provisioner>项目地址</a>。</p><p>NFS Provisioner 也是以 Pod 的形式运行在 Kubernetes 里的，在 GitHub 的 deploy 目录里是部署它所需的 YAML 文件，一共有三个，分别是 rbac.yaml、class.yaml 和 deployment.yaml。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# cat class.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>storage.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>StorageClass</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client</span>
</span></span><span style=display:flex><span><span style=color:#f92672>provisioner</span>: <span style=color:#ae81ff>k8s-sigs.io/nfs-subdir-external-provisioner</span> <span style=color:#75715e># or choose another name, must match deployment&#39;s env PROVISIONER_NAME&#39;</span>
</span></span><span style=display:flex><span><span style=color:#f92672>parameters</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>archiveOnDelete</span>: <span style=color:#e6db74>&#34;false&#34;</span>
</span></span><span style=display:flex><span>  
</span></span></code></pre></div><blockquote><p>archiveOnDelete: &ldquo;false&rdquo; # 自动回收存储空间</p><p>onDelete: &ldquo;retain&rdquo; #暂时保留分配的存储，之后再手动删除</p></blockquote><p>把 namespace: default 全部换成 namespace: kube-system</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>[root@k8s-master ~]# sed -i &#39;s/namespace: default/namespace</span>: <span style=color:#ae81ff>kube-system/g&#39; rbac.yaml</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# cat rbac.yaml </span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># replace with namespace where provisioner is deployed</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRole</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-provisioner-runner</span>
</span></span><span style=display:flex><span><span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;nodes&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;get&#34;</span>, <span style=color:#e6db74>&#34;list&#34;</span>, <span style=color:#e6db74>&#34;watch&#34;</span>]
</span></span><span style=display:flex><span>  - <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;persistentvolumes&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;get&#34;</span>, <span style=color:#e6db74>&#34;list&#34;</span>, <span style=color:#e6db74>&#34;watch&#34;</span>, <span style=color:#e6db74>&#34;create&#34;</span>, <span style=color:#e6db74>&#34;delete&#34;</span>]
</span></span><span style=display:flex><span>  - <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;persistentvolumeclaims&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;get&#34;</span>, <span style=color:#e6db74>&#34;list&#34;</span>, <span style=color:#e6db74>&#34;watch&#34;</span>, <span style=color:#e6db74>&#34;update&#34;</span>]
</span></span><span style=display:flex><span>  - <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;storage.k8s.io&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;storageclasses&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;get&#34;</span>, <span style=color:#e6db74>&#34;list&#34;</span>, <span style=color:#e6db74>&#34;watch&#34;</span>]
</span></span><span style=display:flex><span>  - <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;events&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;create&#34;</span>, <span style=color:#e6db74>&#34;update&#34;</span>, <span style=color:#e6db74>&#34;patch&#34;</span>]
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRoleBinding</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>run-nfs-client-provisioner</span>
</span></span><span style=display:flex><span><span style=color:#f92672>subjects</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># replace with namespace where provisioner is deployed</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
</span></span><span style=display:flex><span><span style=color:#f92672>roleRef</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ClusterRole</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-provisioner-runner</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>apiGroup</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io</span>
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Role</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>leader-locking-nfs-client-provisioner</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># replace with namespace where provisioner is deployed</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
</span></span><span style=display:flex><span><span style=color:#f92672>rules</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>apiGroups</span>: [<span style=color:#e6db74>&#34;&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>resources</span>: [<span style=color:#e6db74>&#34;endpoints&#34;</span>]
</span></span><span style=display:flex><span>    <span style=color:#f92672>verbs</span>: [<span style=color:#e6db74>&#34;get&#34;</span>, <span style=color:#e6db74>&#34;list&#34;</span>, <span style=color:#e6db74>&#34;watch&#34;</span>, <span style=color:#e6db74>&#34;create&#34;</span>, <span style=color:#e6db74>&#34;update&#34;</span>, <span style=color:#e6db74>&#34;patch&#34;</span>]
</span></span><span style=display:flex><span>---
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>RoleBinding</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>leader-locking-nfs-client-provisioner</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># replace with namespace where provisioner is deployed</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
</span></span><span style=display:flex><span><span style=color:#f92672>subjects</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ServiceAccount</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># replace with namespace where provisioner is deployed</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
</span></span><span style=display:flex><span><span style=color:#f92672>roleRef</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Role</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>leader-locking-nfs-client-provisioner</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>apiGroup</span>: <span style=color:#ae81ff>rbac.authorization.k8s.io</span>
</span></span><span style=display:flex><span>  
</span></span></code></pre></div><p>把 namespace: default 改成 namespace: kube-system，修改 volumes 和 env 里的 IP 地址和共享目录名，必须和集群里的 NFS 服务器配置一样。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# cat deployment.yaml </span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>apps/v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Deployment</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>  <span style=color:#75715e># replace with namespace where provisioner is deployed</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>kube-system</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>replicas</span>: <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>strategy</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>type</span>: <span style=color:#ae81ff>Recreate</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>selector</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>matchLabels</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>template</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>labels</span>:
</span></span><span style=display:flex><span>        <span style=color:#f92672>app</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>serviceAccountName</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-provisioner</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>image</span>: <span style=color:#ae81ff>chronolaw/nfs-subdir-external-provisioner:v4.0.2</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-root</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/persistentvolumes</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>PROVISIONER_NAME</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>value</span>: <span style=color:#ae81ff>k8s-sigs.io/nfs-subdir-external-provisioner</span>
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>NFS_SERVER</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>value</span>: <span style=color:#ae81ff>172.21.252.80</span>
</span></span><span style=display:flex><span>            - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>NFS_PATH</span>
</span></span><span style=display:flex><span>              <span style=color:#f92672>value</span>: <span style=color:#ae81ff>/nfs/data</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>        - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nfs-client-root</span>
</span></span><span style=display:flex><span>          <span style=color:#f92672>nfs</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>server</span>: <span style=color:#ae81ff>172.21.252.80</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>path</span>: <span style=color:#ae81ff>/nfs/data</span>
</span></span></code></pre></div><p>先把前面创建的pod pv pvc删除</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pv,pvc</span>
</span></span><span style=display:flex><span>No resources found
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f rbac.yaml </span>
</span></span><span style=display:flex><span>serviceaccount/nfs-client-provisioner created
</span></span><span style=display:flex><span>clusterrole.rbac.authorization.k8s.io/nfs-client-provisioner-runner created
</span></span><span style=display:flex><span>clusterrolebinding.rbac.authorization.k8s.io/run-nfs-client-provisioner created
</span></span><span style=display:flex><span>role.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
</span></span><span style=display:flex><span>rolebinding.rbac.authorization.k8s.io/leader-locking-nfs-client-provisioner created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f class.yaml </span>
</span></span><span style=display:flex><span>storageclass.storage.k8s.io/nfs-client created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f deployment.yaml</span>
</span></span><span style=display:flex><span>deployment.apps/nfs-client-provisioner created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 看到 NFS Provisioner 已经运行起来了</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get deploy -n kube-system</span>
</span></span><span style=display:flex><span>NAME                      READY   UP-TO-DATE   AVAILABLE   AGE
</span></span><span style=display:flex><span>calico-kube-controllers   1/1     <span style=color:#ae81ff>1</span>            <span style=color:#ae81ff>1</span>           27h
</span></span><span style=display:flex><span>coredns                   2/2     <span style=color:#ae81ff>2</span>            <span style=color:#ae81ff>2</span>           27h
</span></span><span style=display:flex><span>nfs-client-provisioner    1/1     <span style=color:#ae81ff>1</span>            <span style=color:#ae81ff>1</span>           106s
</span></span></code></pre></div><p>定义一个 PVC，向系统申请 500MB 的存储空间，使用的 StorageClass 是默认的 nfs-client</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>PersistentVolumeClaim</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>nginx-pvc</span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>accessModes</span>:
</span></span><span style=display:flex><span>    - <span style=color:#ae81ff>ReadWriteMany</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>resources</span>:
</span></span><span style=display:flex><span>    <span style=color:#f92672>requests</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>storage</span>: <span style=color:#ae81ff>500Mi</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>storageClassName</span>: <span style=color:#ae81ff>nfs-client</span>
</span></span><span style=display:flex><span>  
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f mult-pvc.yaml</span>
</span></span><span style=display:flex><span>persistentvolumeclaim/nginx-pvc created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 还是用之前的 nginx-pvc.yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f nginx-pvc.yaml</span>
</span></span><span style=display:flex><span>deployment.apps/nginx-deploy-pvc created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 可以看到NFS Provisioner 自动创建一个PV，大小刚好是500Mi</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pvc</span>
</span></span><span style=display:flex><span>NAME        STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style=display:flex><span>nginx-pvc   Bound    pvc-4589a47a-5e10-444b-b813-6a8d66b39283   500Mi      RWX            nfs-client     35s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pv</span>
</span></span><span style=display:flex><span>NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM               STORAGECLASS   REASON   AGE
</span></span><span style=display:flex><span>pvc-4589a47a-5e10-444b-b813-6a8d66b39283   500Mi      RWX            Delete           Bound    default/nginx-pvc   nfs-client              33s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># NFS 服务器上查看共享目录会多出一个目录</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master data<span style=color:#f92672>]</span><span style=color:#75715e># ls</span>
</span></span><span style=display:flex><span>default-nginx-pvc-pvc-4589a47a-5e10-444b-b813-6a8d66b39283
</span></span></code></pre></div><h2 id=配置管理>配置管理<a hidden class=anchor aria-hidden=true href=#配置管理>#</a></h2><p>应用程序有很多类别的配置信息，但从数据安全的角度来看可以分成两类：</p><ul><li><p>一类是明文配置，也就是不保密，可以任意查询修改，比如服务端口、运行参数、文件路径等等。</p></li><li><p>另一类则是机密配置，由于涉及敏感信息需要保密，不能随便查看，比如密码、密钥、证书等等。</p></li></ul><p>这两类配置信息本质上都是字符串，只是由于安全性的原因，在存放和使用方面有些差异，所以 Kubernetes 也就定义了两个 API 对象，ConfigMap 用来保存明文配置，Secret 用来保存秘密配置。</p><h3 id=configmap>ConfigMap<a hidden class=anchor aria-hidden=true href=#configmap>#</a></h3><p>ConfigMap 是一种 API 对象，用来将非机密性的数据保存到键值对中。使用时， Pods 可以将其用作环境变量、命令行参数或者存储卷中的配置文件。ConfigMap 将你的环境配置信息和容器镜像解耦，便于应用配置的修改。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># 用命令 kubectl create 生成 yaml 模板</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master data]# kubectl create cm info --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>creationTimestamp</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>info</span>
</span></span></code></pre></div><p>ConfigMap 的 YAML 没有 spec 等其他字段，而是用 data 字段</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># 要生成带有 data 字段的 YAML 样板，可以在 kubectl create 后面多加一个参数 --from-literal</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master data]# kubectl create cm conf --from-literal=k=v --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>k</span>: <span style=color:#ae81ff>v</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>creationTimestamp</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#75715e># 增加一些 Key-Value</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>count</span>: <span style=color:#e6db74>&#39;10&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>debug</span>: <span style=color:#e6db74>&#39;on&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>path</span>: <span style=color:#e6db74>&#39;/etc/systemd&#39;</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>greeting</span>: |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span>    <span style=color:#ae81ff>say hello to kubernetes.</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 指定配置文件生成 ConfigMap，--from-file=</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# cat redis.conf</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>appendonly yes</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# kubectl create cm redis.conf --from-file=redis.conf</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>configmap/redis.conf created</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# kubectl get cm redis.conf -o yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>redis.conf</span>: |<span style=color:#e6db74>
</span></span></span><span style=display:flex><span><span style=color:#e6db74>    </span>    <span style=color:#ae81ff>appendonly yes</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>redis.conf</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>namespace</span>: <span style=color:#ae81ff>default</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# kubectl get cm</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>NAME               DATA   AGE</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>conf               0      11h</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>kube-root-ca.crt   1      40h</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>redis.conf         1      2m26s</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 创建 ConfigMap</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f conf.yaml</span>
</span></span><span style=display:flex><span>configmap/info created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看 ConfigMap 的状态</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get cm</span>
</span></span><span style=display:flex><span>NAME               DATA   AGE
</span></span><span style=display:flex><span>conf               <span style=color:#ae81ff>0</span>      11h
</span></span><span style=display:flex><span>info               <span style=color:#ae81ff>4</span>      4s
</span></span><span style=display:flex><span>kube-root-ca.crt   <span style=color:#ae81ff>1</span>      40h
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl describe cm info</span>
</span></span><span style=display:flex><span>Name:         info
</span></span><span style=display:flex><span>Namespace:    default
</span></span><span style=display:flex><span>Labels:       &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:  &lt;none&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Data
</span></span><span style=display:flex><span><span style=color:#f92672>====</span>
</span></span><span style=display:flex><span>debug:
</span></span><span style=display:flex><span>----
</span></span><span style=display:flex><span>on
</span></span><span style=display:flex><span>greeting:
</span></span><span style=display:flex><span>----
</span></span><span style=display:flex><span>say hello to kubernetes.
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>path:
</span></span><span style=display:flex><span>----
</span></span><span style=display:flex><span>/etc/systemd
</span></span><span style=display:flex><span>count:
</span></span><span style=display:flex><span>----
</span></span><span style=display:flex><span><span style=color:#ae81ff>10</span>
</span></span><span style=display:flex><span>Events:  &lt;none&gt;
</span></span></code></pre></div><p>现在 ConfigMap 的 Key-Value 信息就已经存入了 etcd 数据库，后续就可以被其他 API 对象使用。</p><h3 id=secret>Secret<a hidden class=anchor aria-hidden=true href=#secret>#</a></h3><p>Secret 对象类型用来保存敏感信息，例如密码、OAuth 令牌和 SSH 密钥。 将这些信息放在 secret 中比放在 Pod 的定义或者容器镜像中来说更加安全和灵活。</p><p>假设我们需要从自己搭建的私有镜像仓库拉取镜像，比如 Harbor</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl create secret docker-registry harbor-registry <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--docker-server<span style=color:#f92672>=</span>&lt;你的镜像仓库服务器&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--docker-username<span style=color:#f92672>=</span>&lt;你的用户名&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--docker-password<span style=color:#f92672>=</span>&lt;你的密码&gt; <span style=color:#ae81ff>\
</span></span></span><span style=display:flex><span><span style=color:#ae81ff></span>--docker-email<span style=color:#f92672>=</span>&lt;你的邮箱地址&gt;
</span></span></code></pre></div><p>Secret 和 ConfigMap 的结构和用法很类似，不过在 Kubernetes 里 Secret 对象又细分出很多类，比如：</p><ul><li>访问私有镜像仓库的认证信息</li><li>身份识别的凭证信息</li><li>HTTPS 通信的证书和私钥</li><li>一般的机密信息（格式由用户自行解释）</li></ul><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#75715e># 生成 secret YAML 模板文件</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# kubectl create secret generic user --from-literal=name=root --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cm9vdA==	# 做了 Base64 编码</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Secret</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>creationTimestamp</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>user</span>
</span></span><span style=display:flex><span>  
</span></span><span style=display:flex><span><span style=color:#75715e># 增加一些 Key-Value</span>
</span></span><span style=display:flex><span>[<span style=color:#ae81ff>root@k8s-master ~]# kubectl create secret generic user --from-literal=name=root --from-literal=pwd=123456 --from-literal=db=mysql --dry-run=client -o yaml</span>
</span></span><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>data</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>db</span>: <span style=color:#ae81ff>bXlzcWw=		# mysql</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cm9vdA==	# root</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>pwd</span>: <span style=color:#ae81ff>MTIzNDU2		# 123456</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Secret</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>creationTimestamp</span>: <span style=color:#66d9ef>null</span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>user</span>
</span></span></code></pre></div><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 创建 Secret</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f secret.yaml</span>
</span></span><span style=display:flex><span>ksecret/user created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 查看 Secret</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get secret</span>
</span></span><span style=display:flex><span>NAME                  TYPE                                  DATA   AGE
</span></span><span style=display:flex><span>default-token-dvp24   kubernetes.io/service-account-token   <span style=color:#ae81ff>3</span>      40h
</span></span><span style=display:flex><span>user                  Opaque                                <span style=color:#ae81ff>3</span>      13s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># Data 不能直接看到内容，只能看到数据大小</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl describe secret user</span>
</span></span><span style=display:flex><span>Name:         user
</span></span><span style=display:flex><span>Namespace:    default
</span></span><span style=display:flex><span>Labels:       &lt;none&gt;
</span></span><span style=display:flex><span>Annotations:  &lt;none&gt;
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Type:  Opaque
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Data
</span></span><span style=display:flex><span><span style=color:#f92672>====</span>
</span></span><span style=display:flex><span>db:    <span style=color:#ae81ff>5</span> bytes
</span></span><span style=display:flex><span>name:  <span style=color:#ae81ff>4</span> bytes
</span></span><span style=display:flex><span>pwd:   <span style=color:#ae81ff>6</span> bytes
</span></span></code></pre></div><h3 id=使用configmap-和-secret>使用ConfigMap 和 Secret<a hidden class=anchor aria-hidden=true href=#使用configmap-和-secret>#</a></h3><p>ConfigMap 和 Secret 只是一些存储在 etcd 里的字符串，所以如果想要在运行时产生效果，就必须要以某种方式“注入”到 Pod 里，让应用去读取。在这方面的处理上 Kubernetes 和 Docker 是一样的，也是两种途径：<strong>环境变量</strong>和<strong>加载文件</strong>。</p><h4 id=环境变量>环境变量<a hidden class=anchor aria-hidden=true href=#环境变量>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl explain pod.spec.containers.env.valueFrom
</span></span></code></pre></div><p>valueFrom 字段指定了环境变量值的来源，可以是 configMapKeyRef 或者 secretKeyRef ，再进一步指定应用的 ConfigMap/Secret 的 name 和它里面的 key。</p><p>简单示例：</p><p>这里定义了 4 个环境变量，COUNT、GREETING、USERNAME、PASSWORD。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>env-pod</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>env</span>:
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>COUNT</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>valueFrom</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>configMapKeyRef</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>key</span>: <span style=color:#ae81ff>count</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>GREETING</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>valueFrom</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>configMapKeyRef</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>key</span>: <span style=color:#ae81ff>greeting</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>USERNAME</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>valueFrom</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>secretKeyRef</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>user</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>key</span>: <span style=color:#ae81ff>name</span>
</span></span><span style=display:flex><span>      - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>PASSWORD</span>
</span></span><span style=display:flex><span>        <span style=color:#f92672>valueFrom</span>:
</span></span><span style=display:flex><span>          <span style=color:#f92672>secretKeyRef</span>:
</span></span><span style=display:flex><span>            <span style=color:#f92672>name</span>: <span style=color:#ae81ff>user</span>
</span></span><span style=display:flex><span>            <span style=color:#f92672>key</span>: <span style=color:#ae81ff>pwd</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>busybox</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>busy</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>imagePullPolicy</span>: <span style=color:#ae81ff>IfNotPresent</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;/bin/sleep&#34;</span>, <span style=color:#e6db74>&#34;300&#34;</span>]
</span></span></code></pre></div><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/image-20230328112133615.png alt=image-20230328112133615></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#75715e># 创建 Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f env-pod.yaml </span>
</span></span><span style=display:flex><span>pod/env-pod created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME          READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>env-pod       1/1     Running   <span style=color:#ae81ff>0</span>          2m8s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>/ <span style=color:#75715e># echo $COUNT $GREETING</span>
</span></span><span style=display:flex><span><span style=color:#ae81ff>10</span> say hello to kubernetes.
</span></span><span style=display:flex><span>/ <span style=color:#75715e># echo $USERNAME $PASSWORD</span>
</span></span><span style=display:flex><span>root <span style=color:#ae81ff>123456</span>
</span></span></code></pre></div><h4 id=加载文件>加载文件<a hidden class=anchor aria-hidden=true href=#加载文件>#</a></h4><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span>kubectl explain pod.spec.volumes
</span></span></code></pre></div><p>在 spec 里增加一个 volumes 字段，然后再定义卷的名字和引用的 ConfigMap/Secret。</p><p>下面定义两个 Volume，分别引用 ConfigMap 和 Secret，名字是 cm-vol 和 sec-vol：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cm-vol</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>configMap</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>sec-vol</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>secret</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>secretName</span>: <span style=color:#ae81ff>user</span>
</span></span></code></pre></div><p>把定义好的 Volume 挂载到容器里的某个路径下：</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/tmp/cm-items</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cm-vol</span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/tmp/sec-items</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>sec-vol</span>
</span></span></code></pre></div><p>定义 Pod YAML 文件</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-yaml data-lang=yaml><span style=display:flex><span><span style=color:#f92672>apiVersion</span>: <span style=color:#ae81ff>v1</span>
</span></span><span style=display:flex><span><span style=color:#f92672>kind</span>: <span style=color:#ae81ff>Pod</span>
</span></span><span style=display:flex><span><span style=color:#f92672>metadata</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>name</span>: <span style=color:#ae81ff>vol-pod</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>spec</span>:
</span></span><span style=display:flex><span>  <span style=color:#f92672>volumes</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cm-vol</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>configMap</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>info</span>
</span></span><span style=display:flex><span>  - <span style=color:#f92672>name</span>: <span style=color:#ae81ff>sec-vol</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>secret</span>:
</span></span><span style=display:flex><span>      <span style=color:#f92672>secretName</span>: <span style=color:#ae81ff>user</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>  <span style=color:#f92672>containers</span>:
</span></span><span style=display:flex><span>  - <span style=color:#f92672>volumeMounts</span>:
</span></span><span style=display:flex><span>    - <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/tmp/cm-items</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>cm-vol</span>
</span></span><span style=display:flex><span>    - <span style=color:#f92672>mountPath</span>: <span style=color:#ae81ff>/tmp/sec-items</span>
</span></span><span style=display:flex><span>      <span style=color:#f92672>name</span>: <span style=color:#ae81ff>sec-vol</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>image</span>: <span style=color:#ae81ff>busybox</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>name</span>: <span style=color:#ae81ff>busy</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>imagePullPolicy</span>: <span style=color:#ae81ff>IfNotPresent</span>
</span></span><span style=display:flex><span>    <span style=color:#f92672>command</span>: [<span style=color:#e6db74>&#34;/bin/sleep&#34;</span>, <span style=color:#e6db74>&#34;1800&#34;</span>]
</span></span></code></pre></div><p><img loading=lazy src=https://gitee.com/JIAYI233/kabu/raw/master/image-20230328130919037.png alt=image-20230328130919037></p><p>创建Pod，然后进Pod查看挂载路径</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl apply -f vol-pod.yaml</span>
</span></span><span style=display:flex><span>pod/vol-pod created
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl get pod</span>
</span></span><span style=display:flex><span>NAME       READY   STATUS    RESTARTS   AGE
</span></span><span style=display:flex><span>vol-pod    1/1     Running   <span style=color:#ae81ff>0</span>          20s
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl exec -it vol-pod -- sh</span>
</span></span><span style=display:flex><span>/ <span style=color:#75715e># ls /tmp/cm-items/</span>
</span></span><span style=display:flex><span>count     debug     greeting  path
</span></span><span style=display:flex><span>/ <span style=color:#75715e># ls /tmp/sec-items/</span>
</span></span><span style=display:flex><span>db    name  pwd
</span></span><span style=display:flex><span>/ <span style=color:#75715e># cat /tmp/cm-items/greeting</span>
</span></span><span style=display:flex><span>say hello to kubernetes.
</span></span><span style=display:flex><span>/ <span style=color:#75715e># cat /tmp/sec-items/pwd</span>
</span></span><span style=display:flex><span>123456/ <span style=color:#75715e># exit</span>
</span></span></code></pre></div><p>ConfigMap 和 Secret 都变成了目录的形式，而它们里面的 Key-Value 变成了一个个的文件，而文件名就是 Key。以 Volume 的方式来使用 ConfigMap/Secret，就和环境变量不太一样。环境变量用法简单，更适合存放简短的字符串，而 Volume 更适合存放大数据量的配置文件，在 Pod 里加载成文件后让应用直接读取使用。</p><p>修改 ConfigMap/Secret 的 YAML，通过env环境变量的方式加载的配置不会更新，通过volume方式加载的会更新配置。</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-bash data-lang=bash><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl edit cm info</span>
</span></span><span style=display:flex><span>apiVersion: v1
</span></span><span style=display:flex><span>kind: ConfigMap
</span></span><span style=display:flex><span>metadata:
</span></span><span style=display:flex><span>  name: info
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>data:
</span></span><span style=display:flex><span>  count: <span style=color:#e6db74>&#39;1000&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#f92672>[</span>root@k8s-master ~<span style=color:#f92672>]</span><span style=color:#75715e># kubectl exec -it vol-pod -- sh</span>
</span></span><span style=display:flex><span>/ <span style=color:#75715e># cat /tmp/cm-items/count </span>
</span></span><span style=display:flex><span>1000/ <span style=color:#75715e># exit</span>
</span></span></code></pre></div><blockquote><p>如果部署的中间件配置值未更改，是因为Pod部署的中间件自己本身没有热更新能力，需要重新启动 Pod 才能从关联的 ConfigMap 中获取更新的值。</p></blockquote><h2 id=kubesphere>KubeSphere<a hidden class=anchor aria-hidden=true href=#kubesphere>#</a></h2><p>持续更新中&mldr;&mldr;</p></div><footer class=post-footer><ul class=post-tags><li><a href=https://jiayi26.github.io/tags/kubernetes/>Kubernetes</a></li></ul><nav class=paginav><a class=prev href=https://jiayi26.github.io/posts/docker/><span class=title>« Prev Page</span><br><span>Docker</span></a>
<a class=next href=https://jiayi26.github.io/posts/linux%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7/><span class=title>Next Page »</span><br><span>Linux性能分析工具</span></a></nav><div class=share-buttons><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on twitter" href="https://twitter.com/intent/tweet/?text=Kubernetes&url=https%3a%2f%2fjiayi26.github.io%2fposts%2fkubernetes%2f&hashtags=Kubernetes"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM195.519 424.544c135.939.0 210.268-112.643 210.268-210.268.0-3.218.0-6.437-.153-9.502 14.406-10.421 26.973-23.448 36.935-38.314-13.18 5.824-27.433 9.809-42.452 11.648 15.326-9.196 26.973-23.602 32.49-40.92-14.252 8.429-30.038 14.56-46.896 17.931-13.487-14.406-32.644-23.295-53.946-23.295-40.767.0-73.87 33.104-73.87 73.87.0 5.824.613 11.494 1.992 16.858-61.456-3.065-115.862-32.49-152.337-77.241-6.284 10.881-9.962 23.601-9.962 37.088.0 25.594 13.027 48.276 32.95 61.456-12.107-.307-23.448-3.678-33.41-9.196v.92c0 35.862 25.441 65.594 59.311 72.49-6.13 1.686-12.72 2.606-19.464 2.606-4.751.0-9.348-.46-13.946-1.38 9.349 29.426 36.628 50.728 68.965 51.341-25.287 19.771-57.164 31.571-91.8 31.571-5.977.0-11.801-.306-17.625-1.073 32.337 21.15 71.264 33.41 112.95 33.41z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&url=https%3a%2f%2fjiayi26.github.io%2fposts%2fkubernetes%2f&title=Kubernetes&summary=Kubernetes&source=https%3a%2f%2fjiayi26.github.io%2fposts%2fkubernetes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on reddit" href="https://reddit.com/submit?url=https%3a%2f%2fjiayi26.github.io%2fposts%2fkubernetes%2f&title=Kubernetes"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on facebook" href="https://facebook.com/sharer/sharer.php?u=https%3a%2f%2fjiayi26.github.io%2fposts%2fkubernetes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on whatsapp" href="https://api.whatsapp.com/send?text=Kubernetes%20-%20https%3a%2f%2fjiayi26.github.io%2fposts%2fkubernetes%2f"><svg viewBox="0 0 512 512"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a><a target=_blank rel="noopener noreferrer" aria-label="share Kubernetes on telegram" href="https://telegram.me/share/url?text=Kubernetes&url=https%3a%2f%2fjiayi26.github.io%2fposts%2fkubernetes%2f"><svg viewBox="2 2 28 28"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></div></footer></article></main><footer class=footer><span>&copy; 2023 <a href=https://jiayi26.github.io/>JIAYI's Blog</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://git.io/hugopapermod rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg></a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(t){t.preventDefault();var e=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(e)}']`).scrollIntoView({behavior:"smooth"}),e==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${e}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>